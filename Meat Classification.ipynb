{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d9dcd41",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, accuracy_score, precision_score, recall_score, f1_score\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytorch'"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pytorch as torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caab55d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label_encoding = {\n",
    "    'SPOILED': 0,\n",
    "    'HALF': 1,\n",
    "    'FRESH': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c33ec2a",
   "metadata": {},
   "source": [
    "# Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed8098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(file_path, output_x, output_y):\n",
    "    for file_name in os.listdir(file_path):\n",
    "        class_name = file_name.split('-')[0]\n",
    "        if (class_name == '_classes.csv'): continue\n",
    "        img = cv.imread(file_path + file_name).astype('float32')\n",
    "        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "        img = cv.resize(img, (128, 128), interpolation = cv.INTER_AREA)\n",
    "        img /= 255\n",
    "        output_x.append(img)\n",
    "        output_y.append(class_label_encoding[class_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ac2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "test_x = []\n",
    "test_y = []\n",
    "\n",
    "load_images('data/train/', train_x, train_y)\n",
    "load_images('data/valid/', test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2106746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7633df5c",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d60903",
   "metadata": {},
   "source": [
    "## Color Histogram\n",
    "Jason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f6d114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_color_histogram(image, bins=32):\n",
    "    \"\"\"\n",
    "    Extract color histogram features from an image.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: The input image (should be in RGB format)\n",
    "    - bins: Number of bins for the histogram\n",
    "    \n",
    "    Returns:\n",
    "    - histogram_features: Flattened histogram features\n",
    "    \"\"\"\n",
    "    # Extract histograms for each channel\n",
    "    hist_r = cv.calcHist([image], [0], None, [bins], [0, 1])  # Changed range to [0, 1] since you're normalizing images\n",
    "    hist_g = cv.calcHist([image], [1], None, [bins], [0, 1])\n",
    "    hist_b = cv.calcHist([image], [2], None, [bins], [0, 1])\n",
    "    \n",
    "    # Normalize the histograms\n",
    "    cv.normalize(hist_r, hist_r, 0, 1, cv.NORM_MINMAX)\n",
    "    cv.normalize(hist_g, hist_g, 0, 1, cv.NORM_MINMAX)\n",
    "    cv.normalize(hist_b, hist_b, 0, 1, cv.NORM_MINMAX)\n",
    "    \n",
    "    # Flatten and concatenate the histograms\n",
    "    histogram_features = np.concatenate([\n",
    "        hist_r.flatten(), \n",
    "        hist_g.flatten(), \n",
    "        hist_b.flatten()\n",
    "    ])\n",
    "    \n",
    "    return histogram_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69713c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_color_histogram(image, bins=32, title=\"Color Histogram\"):\n",
    "    \"\"\"\n",
    "    Plot the color histogram of an image.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: The input image (should be in RGB format)\n",
    "    - bins: Number of bins for the histogram\n",
    "    - title: Title for the plot\n",
    "    \n",
    "    Returns:\n",
    "    - None (displays the plot)\n",
    "    \"\"\"\n",
    "    # Create a figure with subplots\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    # Display the original image\n",
    "    ax[0].imshow(image)\n",
    "    ax[0].set_title('Original Image')\n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    # Get histogram features using your existing function\n",
    "    features = extract_color_histogram(image, bins)\n",
    "    \n",
    "    # Split the features back into channels\n",
    "    channel_length = len(features) // 3\n",
    "    hist_r = features[:channel_length].reshape(bins, 1)\n",
    "    hist_g = features[channel_length:2*channel_length].reshape(bins, 1)\n",
    "    hist_b = features[2*channel_length:].reshape(bins, 1)\n",
    "    \n",
    "    # Define colors and channels\n",
    "    colors = ['r', 'g', 'b']\n",
    "    channels = ['Red', 'Green', 'Blue']\n",
    "    hists = [hist_r, hist_g, hist_b]\n",
    "    \n",
    "    # Plot histograms for each channel\n",
    "    for i, (hist, col, chan) in enumerate(zip(hists, colors, channels)):\n",
    "        ax[i+1].plot(hist, color=col)\n",
    "        ax[i+1].set_xlim([0, bins])\n",
    "        ax[i+1].set_title(f'{chan} Histogram')\n",
    "        ax[i+1].set_xlabel('Bins')\n",
    "        ax[i+1].set_ylabel('# of Pixels')\n",
    "        ax[i+1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c913fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = train_x[0]  # Get the first image\n",
    "plot_color_histogram(image, bins=32, title=\"Meat Sample Color Histogram\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16684164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from training and testing sets\n",
    "train_features = []\n",
    "for img in train_x:\n",
    "    hist_features = extract_color_histogram(img)\n",
    "    train_features.append(hist_features)\n",
    "train_features = np.array(train_features)\n",
    "\n",
    "\n",
    "test_features = []\n",
    "for img in test_x:\n",
    "    hist_features = extract_color_histogram(img)\n",
    "    test_features.append(hist_features)\n",
    "test_features = np.array(test_features)\n",
    "\n",
    "print(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c288bd",
   "metadata": {},
   "source": [
    "## Local Binary Pattern\n",
    "Aiden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0966dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel(img, center, x, y): \n",
    "      \n",
    "    new_value = 0\n",
    "      \n",
    "    try: \n",
    "        # if local neighbourhood pixel value is greater than or equal to center pixel values then set it to 1 \n",
    "        if img[x][y] >= center: \n",
    "            new_value = 1\n",
    "              \n",
    "    except: \n",
    "        # exception required when neighbourhood value of center pixel value is null\n",
    "        pass\n",
    "      \n",
    "    return new_value \n",
    "   \n",
    "# Function for calculating LBP \n",
    "def lbp_calculated_pixel(img, x, y): \n",
    "   \n",
    "    center = img[x][y] \n",
    "   \n",
    "    val_ar = [] \n",
    "      \n",
    "    # top_left \n",
    "    val_ar.append(get_pixel(img, center, x-1, y-1)) \n",
    "      \n",
    "    # top \n",
    "    val_ar.append(get_pixel(img, center, x-1, y)) \n",
    "      \n",
    "    # top_right \n",
    "    val_ar.append(get_pixel(img, center, x-1, y + 1)) \n",
    "      \n",
    "    # right \n",
    "    val_ar.append(get_pixel(img, center, x, y + 1)) \n",
    "      \n",
    "    # bottom_right \n",
    "    val_ar.append(get_pixel(img, center, x + 1, y + 1)) \n",
    "      \n",
    "    # bottom \n",
    "    val_ar.append(get_pixel(img, center, x + 1, y)) \n",
    "      \n",
    "    # bottom_left \n",
    "    val_ar.append(get_pixel(img, center, x + 1, y-1)) \n",
    "      \n",
    "    # left \n",
    "    val_ar.append(get_pixel(img, center, x, y-1)) \n",
    "       \n",
    "    # convert binary values to decimal \n",
    "    power_val = [1, 2, 4, 8, 16, 32, 64, 128] \n",
    "   \n",
    "    val = 0\n",
    "      \n",
    "    for i in range(len(val_ar)): \n",
    "        val += val_ar[i] * power_val[i] \n",
    "          \n",
    "    return val\n",
    "\n",
    "\n",
    "def lbp_output(img_bgr):\n",
    "    height, width, _ = img_bgr.shape \n",
    "   \n",
    "    # convert RGB to gray \n",
    "    img_gray = cv.cvtColor(img_bgr, \n",
    "                            cv.COLOR_BGR2GRAY) \n",
    "       \n",
    "    # create numpy array as same height and width of RGB image \n",
    "    img_lbp = np.zeros((height, width), \n",
    "                       np.float32) \n",
    "       \n",
    "    for i in range(0, height): \n",
    "        for j in range(0, width): \n",
    "            img_lbp[i, j] = lbp_calculated_pixel(img_gray, i, j)\n",
    "\n",
    "    return img_lbp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9387764",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_bgr = train_x[0]\n",
    "img_lbp = lbp_output(img_bgr)\n",
    "  \n",
    "plt.imshow(img_bgr) \n",
    "plt.show()\n",
    "   \n",
    "plt.imshow(img_lbp, cmap =\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab348b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images_lbp(imgs, labels, train_test='train'):\n",
    "    label_text = ['SPOILED', 'HALF', 'FRESH']\n",
    "    for image in range(len(imgs)):\n",
    "        lbp_image = lbp_output(imgs[image])\n",
    "        filename = f'data/lbp/{train_test}/{label_text[labels[image]]}-{image}-lbp.jpg'\n",
    "        cv.imwrite(filename, lbp_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f9da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_images_lbp(train_x, train_y, train_test='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff94ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_images_lbp(test_x, test_y, train_test='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f715356b",
   "metadata": {},
   "source": [
    "## Histograms of Oriented Gradients\n",
    "Fiona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5931aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from here: https://scikit-image.org/docs/stable/auto_examples/features_detection/plot_hog.html\n",
    "def make_hog(image, visualize=False):\n",
    "    if visualize:\n",
    "        features, hog_image = hog(\n",
    "            image,\n",
    "            orientations=16,\n",
    "            pixels_per_cell=(8, 8),\n",
    "            cells_per_block=(1, 1),\n",
    "            visualize=visualize,\n",
    "            channel_axis=-1\n",
    "        )\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "\n",
    "        ax1.axis('off')\n",
    "        ax1.imshow(image, cmap=plt.cm.gray)\n",
    "        ax1.set_title('Input image')\n",
    "\n",
    "        hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "\n",
    "        ax2.axis('off')\n",
    "        ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
    "        ax2.set_title('Histogram of Oriented Gradients')\n",
    "        plt.show()\n",
    "        return features\n",
    "    else:\n",
    "        features = hog(\n",
    "            image,\n",
    "            orientations=16,\n",
    "            pixels_per_cell=(8, 8),\n",
    "            cells_per_block=(1, 1),\n",
    "            visualize=visualize,\n",
    "            channel_axis=-1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19806fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_hog(train_x[1], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b7c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_hog = []\n",
    "test_features_hog = []\n",
    "\n",
    "for image in train_x:\n",
    "    train_features_hog.append(make_hog(image))\n",
    "\n",
    "for image in test_x:\n",
    "    test_features_hog.append(make_hog(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12472a7b",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3fac7a",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "Jason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4491c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_tree(x_train_tree, y_train_tree, x_test_tree, y_test_tree, max_depth=5, show_tree=True, feature_names=None):\n",
    "    \"\"\"\n",
    "    Train a decision tree classifier on any type of features, with optional histogram visualization.\n",
    "    \n",
    "    Parameters:\n",
    "    - x_train_tree: Training features\n",
    "    - y_train_tree: Training labels\n",
    "    - x_test_tree: Test features\n",
    "    - y_test_tree: Test labels\n",
    "    - max_depth: Maximum depth of the decision tree\n",
    "    - show_tree: Whether to visualize the decision tree\n",
    "    - feature_names: Names of features (will be auto-generated if None)\n",
    "    \n",
    "    Returns:\n",
    "    - dt_classifier: Trained decision tree classifier\n",
    "    - accuracy: Classification accuracy on test set\n",
    "    - report: Classification report\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a list of class names\n",
    "    class_names=['SPOILED', 'HALF', 'FRESH']\n",
    "    \n",
    "    # Create and train a Decision Tree classifier\n",
    "    dt_classifier = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "    dt_classifier.fit(x_train_tree, y_train_tree)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = dt_classifier.predict(x_test_tree)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test_tree, predictions)\n",
    "    report = classification_report(y_test_tree, predictions, target_names=class_names)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Decision Tree Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Show decision tree if requested\n",
    "    if show_tree:\n",
    "        # Create feature names if not provided\n",
    "        if feature_names is None:\n",
    "            feature_names = [f\"Feature_{i}\" for i in range(x_train_tree.shape[1])]\n",
    "            \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plot_tree(dt_classifier, \n",
    "                  feature_names=feature_names,\n",
    "                  class_names=class_names,\n",
    "                  filled=True, \n",
    "                  rounded=True, \n",
    "                  fontsize=8)\n",
    "        plt.title(\"Decision Tree for Classification\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return dt_classifier, accuracy, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e749fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature names for the histogram features\n",
    "bins_per_channel = train_features.shape[1] // 3\n",
    "channels = ['Red', 'Green', 'Blue']\n",
    "feature_names = []\n",
    "for channel in channels:\n",
    "    for index in range(bins_per_channel):\n",
    "        feature_names.append(f\"{channel} Bin {index}\")\n",
    "\n",
    "#x_train_tree, y_train_tree, x_test_tree, y_test_tree, max_depth=5, show_tree=True, feature_names=None\n",
    "\n",
    "# Train the decision tree with histogram visualization\n",
    "model, acc, report = train_decision_tree(x_train_tree=train_features, y_train_tree=train_y, x_test_tree=test_features,\n",
    "                                         y_test_tree=test_y, max_depth=3, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c27a7c6",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Aiden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9584d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(x_train_forest, y_train_forest, x_test_forest, y_test_forest, n_estimators=100, criterion='gini', max_depth=None,\n",
    "                        min_samples_split=2, min_samples_leaf=1, max_features='sqrt'):\n",
    "    # Create Random Forest classifer object\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split,\n",
    "                                min_samples_leaf=min_samples_leaf, max_features=max_features, random_state=42)\n",
    "    \n",
    "    # Train Random Forest Classifer\n",
    "    clf.fit(x_train_forest,y_train_forest)\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(x_test_forest)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_forest, y_pred)\n",
    "    precision = precision_score(y_test_forest, y_pred)\n",
    "    recall = recall_score(y_test_forest, y_pred)\n",
    "    f1 = f1_score(y_test_forest, y_pred)\n",
    "    confusion = confusion_matrix(y_test_forest, y_pred)\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02c26a2-aa0e-4777-ae2b-4b6ff42f6b7f",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16f1cb6-a13d-4d2a-8bff-5b1b7027ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FeedforwardNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A configurable feedforward neural network implemented with PyTorch\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_layers=[64, 32], output_classes=3, \n",
    "                 dropout_rate=0.2, activation='relu', batch_norm=True):\n",
    "        \"\"\"\n",
    "        Initialize the neural network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_dim : int\n",
    "            Number of input features\n",
    "        hidden_layers : list of int\n",
    "            List containing the number of neurons in each hidden layer\n",
    "        output_classes : int\n",
    "            Number of output classes (3 for SPOILED, HALF, FRESH)\n",
    "        dropout_rate : float\n",
    "            Dropout rate for regularization\n",
    "        activation : str\n",
    "            Activation function ('relu', 'sigmoid', 'tanh')\n",
    "        batch_norm : bool\n",
    "            Whether to use batch normalization\n",
    "        \"\"\"\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        \n",
    "        # Choose activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation_fn = nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation_fn = nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation_fn = nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "        \n",
    "        # Build network architecture\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for i, units in enumerate(hidden_layers):\n",
    "            # Add linear layer\n",
    "            layers.append(nn.Linear(prev_dim, units))\n",
    "            \n",
    "            # Add batch normalization if enabled\n",
    "            if batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(units))\n",
    "            \n",
    "            # Add activation function\n",
    "            layers.append(self.activation_fn)\n",
    "            \n",
    "            # Add dropout if enabled\n",
    "            if dropout_rate > 0:\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "            \n",
    "            prev_dim = units\n",
    "        \n",
    "        # Add output layer\n",
    "        layers.append(nn.Linear(prev_dim, output_classes))\n",
    "        \n",
    "        # Create sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class MeatClassifier:\n",
    "    \"\"\"\n",
    "    A wrapper class for training and evaluating the PyTorch neural network model.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_layers=[64, 32], output_classes=3, \n",
    "                 dropout_rate=0.2, activation='relu', batch_norm=True,\n",
    "                 learning_rate=0.001, weight_decay=0.001, device=None):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_dim : int\n",
    "            Number of input features\n",
    "        hidden_layers : list of int\n",
    "            List containing the number of neurons in each hidden layer\n",
    "        output_classes : int\n",
    "            Number of output classes (3 for SPOILED, HALF, FRESH)\n",
    "        dropout_rate : float\n",
    "            Dropout rate for regularization\n",
    "        activation : str\n",
    "            Activation function ('relu', 'sigmoid', 'tanh')\n",
    "        batch_norm : bool\n",
    "            Whether to use batch normalization\n",
    "        learning_rate : float\n",
    "            Learning rate for optimizer\n",
    "        weight_decay : float\n",
    "            Weight decay (L2 regularization) strength\n",
    "        device : str or None\n",
    "            Device to use ('cuda' or 'cpu'). If None, will use CUDA if available.\n",
    "        \"\"\"\n",
    "        # Set device\n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = FeedforwardNN(\n",
    "            input_dim=input_dim,\n",
    "            hidden_layers=hidden_layers,\n",
    "            output_classes=output_classes,\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=activation,\n",
    "            batch_norm=batch_norm\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.model.parameters(), \n",
    "            lr=learning_rate, \n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        \n",
    "        # Initialize loss function (CrossEntropyLoss includes softmax)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # For tracking training progress\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "        \n",
    "        # For storing the best model\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_model_state = None\n",
    "        \n",
    "        # For feature scaling\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    def train(self, features, labels, test_size=0.2, batch_size=32, epochs=100, \n",
    "              early_stopping_patience=20, random_state=42, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the neural network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        features : numpy array\n",
    "            Feature matrix\n",
    "        labels : numpy array\n",
    "            Labels\n",
    "        test_size : float\n",
    "            Proportion of data to use for validation\n",
    "        batch_size : int\n",
    "            Batch size for training\n",
    "        epochs : int\n",
    "            Number of training epochs\n",
    "        early_stopping_patience : int\n",
    "            Number of epochs with no improvement after which training will stop\n",
    "        random_state : int\n",
    "            Random seed for reproducibility\n",
    "        verbose : bool\n",
    "            Whether to print training progress\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : MeatClassifier\n",
    "            The trained classifier\n",
    "        \"\"\"\n",
    "        # Convert to numpy arrays if not already\n",
    "        features = np.array(features)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        # Split data into train and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            features, labels, test_size=test_size, \n",
    "            random_state=random_state, stratify=labels\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        X_train = self.scaler.fit_transform(X_train)\n",
    "        X_val = self.scaler.transform(X_val)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(self.device)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(self.device)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(self.device)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(self.device)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        \n",
    "        # For early stopping\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "            \n",
    "            for inputs, targets in train_loader:\n",
    "                # Forward pass\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # Track statistics\n",
    "                train_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_train += targets.size(0)\n",
    "                correct_train += (predicted == targets).sum().item()\n",
    "            \n",
    "            # Calculate training metrics\n",
    "            epoch_train_loss = train_loss / len(train_loader)\n",
    "            epoch_train_acc = correct_train / total_train\n",
    "            self.train_losses.append(epoch_train_loss)\n",
    "            self.train_accuracies.append(epoch_train_acc)\n",
    "            \n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct_val = 0\n",
    "            total_val = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in val_loader:\n",
    "                    # Forward pass\n",
    "                    outputs = self.model(inputs)\n",
    "                    loss = self.criterion(outputs, targets)\n",
    "                    \n",
    "                    # Track statistics\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total_val += targets.size(0)\n",
    "                    correct_val += (predicted == targets).sum().item()\n",
    "            \n",
    "            # Calculate validation metrics\n",
    "            epoch_val_loss = val_loss / len(val_loader)\n",
    "            epoch_val_acc = correct_val / total_val\n",
    "            self.val_losses.append(epoch_val_loss)\n",
    "            self.val_accuracies.append(epoch_val_acc)\n",
    "            \n",
    "            # Save the best model\n",
    "            if epoch_val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = epoch_val_loss\n",
    "                self.best_model_state = self.model.state_dict().copy()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}: \"\n",
    "                      f\"train_loss={epoch_train_loss:.4f}, \"\n",
    "                      f\"train_acc={epoch_train_acc:.4f}, \"\n",
    "                      f\"val_loss={epoch_val_loss:.4f}, \"\n",
    "                      f\"val_acc={epoch_val_acc:.4f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if early_stopping_patience > 0 and patience_counter >= early_stopping_patience:\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        # Load the best model\n",
    "        if self.best_model_state is not None:\n",
    "            self.model.load_state_dict(self.best_model_state)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, features):\n",
    "        \"\"\"\n",
    "        Make predictions on new data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        features : numpy array\n",
    "            Feature matrix\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        numpy array\n",
    "            Predicted class indices\n",
    "        \"\"\"\n",
    "        # Scale features\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        \n",
    "        # Convert to PyTorch tensor\n",
    "        X_tensor = torch.tensor(features_scaled, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # Make predictions\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "        \n",
    "        return predictions.cpu().numpy()\n",
    "    \n",
    "    def predict_proba(self, features):\n",
    "        \"\"\"\n",
    "        Predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        features : numpy array\n",
    "            Feature matrix\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        numpy array\n",
    "            Class probabilities\n",
    "        \"\"\"\n",
    "        # Scale features\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        \n",
    "        # Convert to PyTorch tensor\n",
    "        X_tensor = torch.tensor(features_scaled, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # Make predictions\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "        \n",
    "        return probabilities.cpu().numpy()\n",
    "    \n",
    "    def evaluate(self, features, labels):\n",
    "        \"\"\"\n",
    "        Evaluate the model on test data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        features : numpy array\n",
    "            Feature matrix\n",
    "        labels : numpy array\n",
    "            True labels\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary containing evaluation metrics\n",
    "        \"\"\"\n",
    "        # Get predictions\n",
    "        y_pred = self.predict(features)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(labels, y_pred)\n",
    "        \n",
    "        # Generate classification report\n",
    "        class_names = ['SPOILED', 'HALF', 'FRESH']\n",
    "        report = classification_report(labels, y_pred, target_names=class_names, output_dict=True)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'report': report\n",
    "        }\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot the training history (loss and accuracy curves).\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Loss plot\n",
    "        ax1.plot(self.train_losses, label='Training Loss')\n",
    "        ax1.plot(self.val_losses, label='Validation Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Loss Curves')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracy plot\n",
    "        ax2.plot(self.train_accuracies, label='Training Accuracy')\n",
    "        ax2.plot(self.val_accuracies, label='Validation Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.set_title('Accuracy Curves')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the model to a file.\"\"\"\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scaler': self.scaler,\n",
    "            'best_val_loss': self.best_val_loss\n",
    "        }, filepath)\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load the model from a file.\"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scaler = checkpoint['scaler']\n",
    "        self.best_val_loss = checkpoint['best_val_loss']\n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "def train_meat_classifier_pytorch(features, labels, hidden_layers=[128, 64], \n",
    "                                  learning_rate=0.001, weight_decay=0.001,\n",
    "                                  batch_size=32, epochs=100, test_size=0.2,\n",
    "                                  early_stopping_patience=20, dropout_rate=0.2,\n",
    "                                  activation='relu', batch_norm=True,\n",
    "                                  random_state=42, verbose=True):\n",
    "    \"\"\"\n",
    "    Train and evaluate a neural network for meat classification using PyTorch.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    features : numpy array\n",
    "        Feature matrix (color histogram, HOG, or LBP features)\n",
    "    labels : numpy array\n",
    "        Labels (0: SPOILED, 1: HALF, 2: FRESH)\n",
    "    hidden_layers : list of int\n",
    "        List of hidden layer sizes\n",
    "    learning_rate : float\n",
    "        Learning rate for optimizer\n",
    "    weight_decay : float\n",
    "        Weight decay (L2 regularization) strength\n",
    "    batch_size : int\n",
    "        Batch size for training\n",
    "    epochs : int\n",
    "        Maximum number of epochs for training\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    early_stopping_patience : int\n",
    "        Number of epochs with no improvement after which training will stop\n",
    "    dropout_rate : float\n",
    "        Dropout rate for regularization\n",
    "    activation : str\n",
    "        Activation function ('relu', 'sigmoid', 'tanh')\n",
    "    batch_norm : bool\n",
    "        Whether to use batch normalization\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    verbose : bool\n",
    "        Whether to print training progress\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing the classifier, evaluation metrics, and other info\n",
    "    \"\"\"\n",
    "    # Create classifier\n",
    "    classifier = MeatClassifier(\n",
    "        input_dim=features.shape[1],\n",
    "        hidden_layers=hidden_layers,\n",
    "        output_classes=len(np.unique(labels)),\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=activation,\n",
    "        batch_norm=batch_norm,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    # Train the classifier\n",
    "    classifier.train(\n",
    "        features=features,\n",
    "        labels=labels,\n",
    "        test_size=test_size,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        early_stopping_patience=early_stopping_patience,\n",
    "        random_state=random_state,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Split data for evaluation (using the same random state for consistency)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, labels, test_size=test_size, \n",
    "        random_state=random_state, stratify=labels\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    evaluation = classifier.evaluate(X_test, y_test)\n",
    "    \n",
    "    # Plot training history\n",
    "    classifier.plot_training_history()\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'classifier': classifier,\n",
    "        'accuracy': evaluation['accuracy'],\n",
    "        'report': evaluation['report']\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# For color histogram features\n",
    "color_hist_results = train_meat_classifier_pytorch(\n",
    "    features=np.array(train_features),  # Assuming this is your color histogram features\n",
    "    labels=np.array(train_y),\n",
    "    hidden_layers=[128, 64, 32],\n",
    "    epochs=150,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Print accuracy and classification report\n",
    "print(f\"Test accuracy: {color_hist_results['accuracy']:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "for class_name, metrics in color_hist_results['report'].items():\n",
    "    if isinstance(metrics, dict):  # Skip aggregated metrics\n",
    "        print(f\"{class_name}: Precision={metrics['precision']:.2f}, Recall={metrics['recall']:.2f}, F1-score={metrics['f1-score']:.2f}\")\n",
    "\n",
    "# For HOG features\n",
    "hog_results = train_meat_classifier_pytorch(\n",
    "    features=np.array(train_features_hog),  # Assuming this is your HOG features\n",
    "    labels=np.array(train_y),\n",
    "    hidden_layers=[64, 32],\n",
    "    epochs=150,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
