{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09d371ea",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9dcd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, regularizers\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2771d30d",
   "metadata": {},
   "source": [
    "# Import caching utility functions from local module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15cb041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload module when changes are made\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import caching_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caab55d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label_encoding = {\n",
    "    'SPOILED': 0,\n",
    "    'HALF': 1,\n",
    "    'FRESH': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c33ec2a",
   "metadata": {},
   "source": [
    "# Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed8098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(file_path, output_x, output_y):\n",
    "    for file_name in os.listdir(file_path):\n",
    "        class_name = file_name.split('-')[0]\n",
    "        if (class_name == '_classes.csv'): continue\n",
    "        img = cv.imread(file_path + file_name).astype('float32')\n",
    "        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "        img = cv.resize(img, (128, 128), interpolation = cv.INTER_AREA)\n",
    "        img /= 255\n",
    "        output_x.append(img)\n",
    "        output_y.append(class_label_encoding[class_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ac2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "test_x = []\n",
    "test_y = []\n",
    "\n",
    "load_images('data/train/', train_x, train_y)\n",
    "load_images('data/valid/', test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2106746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7633df5c",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d60903",
   "metadata": {},
   "source": [
    "## Color Histogram\n",
    "Jason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f6d114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_color_histogram(image, bins=32):\n",
    "    \"\"\"\n",
    "    Extract color histogram features from an image.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: The input image (should be in RGB format)\n",
    "    - bins: Number of bins for the histogram\n",
    "    \n",
    "    Returns:\n",
    "    - histogram_features: Flattened histogram features\n",
    "    \"\"\"\n",
    "    # Extract histograms for each channel\n",
    "    hist_r = cv.calcHist([image], [0], None, [bins], [0, 1])  # Changed range to [0, 1] since you're normalizing images\n",
    "    hist_g = cv.calcHist([image], [1], None, [bins], [0, 1])\n",
    "    hist_b = cv.calcHist([image], [2], None, [bins], [0, 1])\n",
    "    \n",
    "    # Normalize the histograms\n",
    "    cv.normalize(hist_r, hist_r, 0, 1, cv.NORM_MINMAX)\n",
    "    cv.normalize(hist_g, hist_g, 0, 1, cv.NORM_MINMAX)\n",
    "    cv.normalize(hist_b, hist_b, 0, 1, cv.NORM_MINMAX)\n",
    "    \n",
    "    # Flatten and concatenate the histograms\n",
    "    histogram_features = np.concatenate([\n",
    "        hist_r.flatten(), \n",
    "        hist_g.flatten(), \n",
    "        hist_b.flatten()\n",
    "    ])\n",
    "    \n",
    "    return histogram_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69713c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_color_histogram(image, bins=32, title=\"Color Histogram\"):\n",
    "    \"\"\"\n",
    "    Plot the color histogram of an image.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: The input image (should be in RGB format)\n",
    "    - bins: Number of bins for the histogram\n",
    "    - title: Title for the plot\n",
    "    \n",
    "    Returns:\n",
    "    - None (displays the plot)\n",
    "    \"\"\"\n",
    "    # Create a figure with subplots\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    # Display the original image\n",
    "    ax[0].imshow(image)\n",
    "    ax[0].set_title('Original Image')\n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    # Get histogram features using your existing function\n",
    "    features = extract_color_histogram(image, bins)\n",
    "    \n",
    "    # Split the features back into channels\n",
    "    channel_length = len(features) // 3\n",
    "    hist_r = features[:channel_length].reshape(bins, 1)\n",
    "    hist_g = features[channel_length:2*channel_length].reshape(bins, 1)\n",
    "    hist_b = features[2*channel_length:].reshape(bins, 1)\n",
    "    \n",
    "    # Define colors and channels\n",
    "    colors = ['r', 'g', 'b']\n",
    "    channels = ['Red', 'Green', 'Blue']\n",
    "    hists = [hist_r, hist_g, hist_b]\n",
    "    \n",
    "    # Plot histograms for each channel\n",
    "    for i, (hist, col, chan) in enumerate(zip(hists, colors, channels)):\n",
    "        ax[i+1].plot(hist, color=col)\n",
    "        ax[i+1].set_xlim([0, bins])\n",
    "        ax[i+1].set_title(f'{chan} Histogram')\n",
    "        ax[i+1].set_xlabel('Bins')\n",
    "        ax[i+1].set_ylabel('# of Pixels')\n",
    "        ax[i+1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c913fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = train_x[0]  # Get the first image\n",
    "image_1_filter_1 = plot_color_histogram(image, bins=100, title=\"Meat Sample Color Histogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e4095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_hist(use_cached=True) -> tuple[np.ndarray, np.ndarray]:\n",
    "    train, success = caching_utils.attempt_load_feature_from_cache(\"raw_hist_train.csv\")\n",
    "    if not success or not use_cached: # Not present in cache, regenerate.\n",
    "        train = []\n",
    "        for img in train_x:\n",
    "            hist_features = extract_color_histogram(img)\n",
    "            train.append(hist_features)\n",
    "        train = np.array(train)\n",
    "        caching_utils.save_feature_to_cache(\"raw_hist_train.csv\", train)\n",
    "\n",
    "    test, success = caching_utils.attempt_load_feature_from_cache(\"raw_hist_test.csv\")\n",
    "    if not success or not use_cached:\n",
    "        test = []\n",
    "        for img in test_x:\n",
    "            hist_features = extract_color_histogram(img)\n",
    "            test.append(hist_features)\n",
    "        test = np.array(test)\n",
    "        caching_utils.save_feature_to_cache(\"raw_hist_test.csv\", test)\n",
    "    return (train, test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c288bd",
   "metadata": {},
   "source": [
    "## Local Binary Pattern\n",
    "Aiden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0966dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel(img, center, x, y): \n",
    "      \n",
    "    new_value = 0\n",
    "      \n",
    "    try: \n",
    "        # if local neighbourhood pixel value is greater than or equal to center pixel values then set it to 1 \n",
    "        if img[x][y] >= center: \n",
    "            new_value = 1\n",
    "              \n",
    "    except: \n",
    "        # exception required when neighbourhood value of center pixel value is null\n",
    "        pass\n",
    "      \n",
    "    return new_value \n",
    "   \n",
    "# Function for calculating LBP \n",
    "def lbp_calculated_pixel(img, x, y): \n",
    "   \n",
    "    center = img[x][y] \n",
    "   \n",
    "    val_ar = [] \n",
    "      \n",
    "    # top_left \n",
    "    val_ar.append(get_pixel(img, center, x-1, y-1)) \n",
    "      \n",
    "    # top \n",
    "    val_ar.append(get_pixel(img, center, x-1, y)) \n",
    "      \n",
    "    # top_right \n",
    "    val_ar.append(get_pixel(img, center, x-1, y + 1)) \n",
    "      \n",
    "    # right \n",
    "    val_ar.append(get_pixel(img, center, x, y + 1)) \n",
    "      \n",
    "    # bottom_right \n",
    "    val_ar.append(get_pixel(img, center, x + 1, y + 1)) \n",
    "      \n",
    "    # bottom \n",
    "    val_ar.append(get_pixel(img, center, x + 1, y)) \n",
    "      \n",
    "    # bottom_left \n",
    "    val_ar.append(get_pixel(img, center, x + 1, y-1)) \n",
    "      \n",
    "    # left \n",
    "    val_ar.append(get_pixel(img, center, x, y-1)) \n",
    "       \n",
    "    # convert binary values to decimal \n",
    "    power_val = [1, 2, 4, 8, 16, 32, 64, 128] \n",
    "   \n",
    "    val = 0\n",
    "      \n",
    "    for i in range(len(val_ar)): \n",
    "        val += val_ar[i] * power_val[i] \n",
    "          \n",
    "    return val\n",
    "\n",
    "\n",
    "def lbp_output(img_bgr):\n",
    "    height, width, _ = img_bgr.shape \n",
    "   \n",
    "    # convert RGB to gray \n",
    "    img_gray = cv.cvtColor(img_bgr, \n",
    "                            cv.COLOR_BGR2GRAY) \n",
    "       \n",
    "    # create numpy array as same height and width of RGB image \n",
    "    img_lbp = np.zeros((height, width), \n",
    "                       np.float32) \n",
    "       \n",
    "    for i in range(0, height): \n",
    "        for j in range(0, width): \n",
    "            img_lbp[i, j] = lbp_calculated_pixel(img_gray, i, j)\n",
    "\n",
    "    return img_lbp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9387764",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_bgr = train_x[0]\n",
    "img_lbp = lbp_output(img_bgr)\n",
    "  \n",
    "plt.imshow(img_bgr) \n",
    "plt.show()\n",
    "   \n",
    "plt.imshow(img_lbp, cmap =\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab348b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images_lbp(imgs, labels, train_test='train'):\n",
    "    label_text = ['SPOILED', 'HALF', 'FRESH']\n",
    "    for image in range(len(imgs)):\n",
    "        lbp_image = lbp_output(imgs[image])\n",
    "        filename = f'data/lbp/{train_test}/{label_text[labels[image]]}-{image}-lbp.jpg'\n",
    "        cv.imwrite(filename, lbp_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f9da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_images_lbp(train_x, train_y, train_test='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff94ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_images_lbp(test_x, test_y, train_test='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49772b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_lbp(use_cached=True) -> tuple[np.ndarray, np.ndarray]:\n",
    "    train, success = caching_utils.attempt_load_feature_from_cache(\"raw_lbp_train.csv\")\n",
    "    if not success or not use_cached:\n",
    "        train = []\n",
    "        for img in train_x:\n",
    "            lbp_features = lbp_output(img).flatten()\n",
    "            train.append(lbp_features)\n",
    "        train = np.array(train)\n",
    "        caching_utils.save_feature_to_cache(\"raw_lbp_train.csv\", train)\n",
    "    \n",
    "    test, success = caching_utils.attempt_load_feature_from_cache(\"raw_lbp_test.csv\")\n",
    "    if not success or not use_cached:\n",
    "        test = []\n",
    "        for img in test_x:\n",
    "            lbp_features = lbp_output(img).flatten()\n",
    "            test.append(lbp_features)\n",
    "        test = np.array(test)\n",
    "        caching_utils.save_feature_to_cache(\"raw_lbp_test.csv\", test)\n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f715356b",
   "metadata": {},
   "source": [
    "## Histograms of Oriented Gradients\n",
    "Fiona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5931aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from here: https://scikit-image.org/docs/stable/auto_examples/features_detection/plot_hog.html\n",
    "def make_hog(image, visualize=False):\n",
    "    features, hog_image = hog(\n",
    "            image,\n",
    "            orientations=8,\n",
    "            pixels_per_cell=(16, 16),\n",
    "            cells_per_block=(1, 1),\n",
    "            visualize=True,\n",
    "            channel_axis=-1\n",
    "        )\n",
    "\n",
    "    if visualize:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "\n",
    "        ax1.axis('off')\n",
    "        ax1.imshow(image, cmap=plt.cm.gray)\n",
    "        ax1.set_title('Input image')\n",
    "\n",
    "        hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "\n",
    "        ax2.axis('off')\n",
    "        ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
    "        ax2.set_title('Histogram of Oriented Gradients')\n",
    "        plt.show()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19806fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_hog(train_x[1], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ba9fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_hog(use_cached=True) -> tuple[np.ndarray, np.ndarray]:\n",
    "    train, success = caching_utils.attempt_load_feature_from_cache(\"raw_hog_train.csv\")\n",
    "    if not success or not use_cached:\n",
    "        train = []\n",
    "        for img in train_x:\n",
    "            hog_features = make_hog(img)\n",
    "            train.append(hog_features)\n",
    "        train = np.array(train)\n",
    "        caching_utils.save_feature_to_cache(\"raw_hog_train.csv\", train)\n",
    "\n",
    "    test, success = caching_utils.attempt_load_feature_from_cache(\"raw_hog_test.csv\")\n",
    "    if not success or not use_cached:\n",
    "        test = []\n",
    "        for img in test_x:\n",
    "            hog_features = make_hog(img)\n",
    "            test.append(hog_features)\n",
    "        test = np.array(test)\n",
    "        caching_utils.save_feature_to_cache(\"raw_hog_test.csv\", test)\n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1026b8af",
   "metadata": {},
   "source": [
    "## Load extracted features from cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeee08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_hist, test_features_hist = load_feature_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c67613",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_lbp, test_features_lbp = load_feature_lbp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8cf5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_hog, test_features_hog = load_feature_hog()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feef1f4",
   "metadata": {},
   "source": [
    "# Convert features to dataframe and apply PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aede5e56",
   "metadata": {},
   "source": [
    "## Create Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64005f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_features_to_dataframe(train_features, train_labels, test_features, test_labels,\n",
    "                                    pca_components=2) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Transform a set of extracted features into a pandas DataFrame and applies PCA.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_features: List of extracted features for training (numpy arrays)\n",
    "    - train_labels: Labels for training\n",
    "    - test_features: List of extracted features for training (numpy arrays)\n",
    "    - test_labels: Labels for testing\n",
    "    - pca_components: Number of PCA components to keep (default=2)\n",
    "    - labels: Optional list of labels for the images\n",
    "    \n",
    "    Returns:\n",
    "    - tuple (train_dataframe, test_dataframe)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=pca_components)\n",
    "    pca.fit(train_features)\n",
    "\n",
    "    train_features_pca = pca.transform(train_features)\n",
    "    test_features_pca = pca.transform(test_features)\n",
    "    \n",
    "    # Create DataFrame with PCA results\n",
    "    columns = [f'pca_{i+1}' for i in range(pca_components)]\n",
    "    train_df = pd.DataFrame(train_features_pca, columns=columns)\n",
    "    train_df['label'] = train_labels\n",
    "\n",
    "    test_df = pd.DataFrame(test_features_pca, columns=columns)\n",
    "    test_df['label'] = test_labels\n",
    "    \n",
    "    return (train_df, test_df)\n",
    "\n",
    "\"\"\"\n",
    "Example usage:\n",
    "train, test = transform_features_to_dataframe(x_train, y_train, x_test, y_test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0de3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframe(train_features: np.ndarray, train_labels: np.ndarray,\n",
    "                   test_features: np.ndarray, test_labels: np.ndarray,\n",
    "                   name: str, pca_components: int=2, use_cached=True) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    target = f\"{name}_pca_{pca_components}\"\n",
    "\n",
    "    train, train_success = caching_utils.attempt_load_dataframe(target + \"_train\")\n",
    "    test, test_success = caching_utils.attempt_load_dataframe(target + \"_test\")\n",
    "\n",
    "    if not train_success or not test_success or not use_cached:\n",
    "        train, test = transform_features_to_dataframe(train_features, train_labels, test_features, test_labels, pca_components=pca_components)\n",
    "        caching_utils.save_dataframe_to_cache(target + \"_train\", train)\n",
    "        caching_utils.save_dataframe_to_cache(target + \"_test\", test)\n",
    "    \n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a0811",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_hist, test_df_hist = transform_features_to_dataframe(train_features_hist, train_y, test_features_hist, test_y, pca_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a2efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_hist, test_df_hist = make_dataframe(train_features_hist, train_y, test_features_hist, test_y, \"hist\", pca_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ddd033",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_lbp, test_df_lbp = make_dataframe(train_features_lbp, train_y, test_features_lbp, test_y, \"lbp\", pca_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83461ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_hog, test_df_hog = make_dataframe(train_features_hog, train_y, test_features_hog, test_y, \"hog\", pca_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12472a7b",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3fac7a",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "Jason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4491c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_tree(x_train_tree, y_train_tree, x_test_tree, y_test_tree, max_depth=5, show_tree=True, feature_names=None):\n",
    "    \"\"\"\n",
    "    Train a decision tree classifier on any type of features, with optional histogram visualization.\n",
    "    \n",
    "    Parameters:\n",
    "    - x_train_tree: Training features\n",
    "    - y_train_tree: Training labels\n",
    "    - x_test_tree: Test features\n",
    "    - y_test_tree: Test labels\n",
    "    - max_depth: Maximum depth of the decision tree\n",
    "    - show_tree: Whether to visualize the decision tree\n",
    "    - feature_names: Names of features (will be auto-generated if None)\n",
    "    \n",
    "    Returns:\n",
    "    - dt_classifier: Trained decision tree classifier\n",
    "    - accuracy: Classification accuracy on test set\n",
    "    - report: Classification report\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a list of class names\n",
    "    class_names=['SPOILED', 'HALF', 'FRESH']\n",
    "    \n",
    "    # Create and train a Decision Tree classifier\n",
    "    dt_classifier = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "    dt_classifier.fit(x_train_tree, y_train_tree)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = dt_classifier.predict(x_test_tree)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test_tree, predictions)\n",
    "    report = classification_report(y_test_tree, predictions, target_names=class_names)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Decision Tree Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Show decision tree if requested\n",
    "    if show_tree:\n",
    "        # Create feature names if not provided\n",
    "        if feature_names is None:\n",
    "            feature_names = [f\"Feature_{i}\" for i in range(x_train_tree.shape[1])]\n",
    "            \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plot_tree(dt_classifier, \n",
    "                  feature_names=feature_names,\n",
    "                  class_names=class_names,\n",
    "                  filled=True, \n",
    "                  rounded=True, \n",
    "                  fontsize=8)\n",
    "        plt.title(\"Decision Tree for Classification\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return dt_classifier, accuracy, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e749fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature names for the histogram features\n",
    "bins_per_channel = train_features_hist.shape[1] // 3\n",
    "channels = ['Red', 'Green', 'Blue']\n",
    "feature_names = []\n",
    "for channel in channels:\n",
    "    for index in range(bins_per_channel):\n",
    "        feature_names.append(f\"{channel} Bin {index}\")\n",
    "\n",
    "#x_train_tree, y_train_tree, x_test_tree, y_test_tree, max_depth=5, show_tree=True, feature_names=None\n",
    "\n",
    "# Train the decision tree with histogram visualization\n",
    "model, acc, report = train_decision_tree(x_train_tree=train_features_hist, y_train_tree=train_y, x_test_tree=test_features_hist,\n",
    "                                         y_test_tree=test_y, max_depth=3, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c27a7c6",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Aiden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9584d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(x_train_forest, y_train_forest, x_test_forest, y_test_forest, n_estimators=100, criterion='gini', max_depth=None,\n",
    "                        min_samples_split=2, min_samples_leaf=1, max_features='sqrt'):\n",
    "    # Create Random Forest classifer object\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split,\n",
    "                                min_samples_leaf=min_samples_leaf, max_features=max_features, random_state=42)\n",
    "    \n",
    "    # Train Random Forest Classifer\n",
    "    clf.fit(x_train_forest,y_train_forest)\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(x_test_forest)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_forest, y_pred)\n",
    "    precision = precision_score(y_test_forest, y_pred)\n",
    "    recall = recall_score(y_test_forest, y_pred)\n",
    "    f1 = f1_score(y_test_forest, y_pred)\n",
    "    confusion = confusion_matrix(y_test_forest, y_pred)\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fc0ea5",
   "metadata": {},
   "source": [
    "# Knn\n",
    "Fiona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38cca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knn(x_train, y_train, n_neighbors=5):\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(x_train, y_train)\n",
    "\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e0508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_report(knn, x_test, y_test):\n",
    "    predictions = knn.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    report = classification_report(y_test, predictions, target_names=['SPOILED', 'HALF', 'FRESH'])\n",
    "    print(f\"Knn Accuracy: {accuracy:.4f}\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c609c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from here: https://medium.com/@agrawalsam1997/hyperparameter-tuning-of-knn-classifier-a32f31af25c7\n",
    "def tune_knn(x_train, y_train, x_test, y_test, title,\n",
    "             n_start=1, n_stop=10, n_step=1):\n",
    "    train_scores = {}\n",
    "    test_scores = {}\n",
    "    f1_scores = {}\n",
    "    models = {}\n",
    "\n",
    "    n_neighbors = np.arange(n_start, n_stop, n_step)\n",
    "    for n in n_neighbors:\n",
    "        knn = train_knn(x_train, y_train, n_neighbors=n)\n",
    "        train_scores[n] = knn.score(x_train, y_train)\n",
    "        test_scores[n] = knn.score(x_test, y_test)\n",
    "        f1_scores[n] = f1_score(y_test, knn.predict(x_test), average='macro')\n",
    "        models[n] = knn\n",
    "\n",
    "    plt.plot(n_neighbors, train_scores.values(), label=\"Train Accuracy\")\n",
    "    plt.plot(n_neighbors, test_scores.values(), label=\"Test Accuracy\")\n",
    "    plt.plot(n_neighbors, f1_scores.values(), label=\"F1 Score\", linestyle='--')\n",
    "    plt.xlabel(\"Number of Neighbors\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(f\"KNN ({title}): Varying Number of Neighbors\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a17a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HOG\")\n",
    "knn = train_knn(train_features_hog, train_y)\n",
    "knn_report(knn, test_features_hog, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a58f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_knn(train_df_hist[['pca_1', 'pca_2']], train_df_hist['label'], test_df_hist[['pca_1', 'pca_2']], test_df_hist['label'], \"Color Histograms\", n_stop=50)\n",
    "tune_knn(train_df_hog[['pca_1', 'pca_2']], train_df_hog['label'], test_df_hog[['pca_1', 'pca_2']], test_df_hog['label'], \"HOG\", n_stop=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02c26a2-aa0e-4777-ae2b-4b6ff42f6b7f",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16f1cb6-a13d-4d2a-8bff-5b1b7027ff3c",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "class FeedforwardNN(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    A configurable feedforward neural network implemented with TensorFlow\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_layers=[64, 32], output_classes=3, \n",
    "                 dropout_rate=0.2, activation='relu', batch_norm=True):\n",
    "        \"\"\"\n",
    "        Initialize the neural network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_dim : int\n",
    "            Number of input features\n",
    "        hidden_layers : list of int\n",
    "            List containing the number of neurons in each hidden layer\n",
    "        output_classes : int\n",
    "            Number of output classes (3 for SPOILED, HALF, FRESH)\n",
    "        dropout_rate : float\n",
    "            Dropout rate for regularization\n",
    "        activation : str\n",
    "            Activation function ('relu', 'sigmoid', 'tanh')\n",
    "        batch_norm : bool\n",
    "            Whether to use batch normalization\n",
    "        \"\"\"\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        \n",
    "        # Store activation function name\n",
    "        self.activation_name = activation\n",
    "        \n",
    "        # Build network architecture\n",
    "        self.model_layers = []\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for units in hidden_layers:\n",
    "            # Add linear layer\n",
    "            self.model_layers.append(layers.Dense(units))\n",
    "            \n",
    "            # Add batch normalization if enabled\n",
    "            if batch_norm:\n",
    "                self.model_layers.append(layers.BatchNormalization())\n",
    "            \n",
    "            # Add activation function\n",
    "            self.model_layers.append(layers.Activation(activation))\n",
    "            \n",
    "            # Add dropout if enabled\n",
    "            if dropout_rate > 0:\n",
    "                self.model_layers.append(layers.Dropout(dropout_rate))\n",
    "        \n",
    "        # Add output layer\n",
    "        self.model_layers.append(layers.Dense(output_classes))\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        for layer in self.model_layers:\n",
    "            # For batch normalization and dropout layers, we need to specify training mode\n",
    "            if isinstance(layer, (layers.BatchNormalization, layers.Dropout)):\n",
    "                x = layer(x, training=training)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MeatClassifier:\n",
    "    \"\"\"\n",
    "    A wrapper class for training and evaluating the TensorFlow neural network model.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_layers=[64, 32], output_classes=3, \n",
    "                 dropout_rate=0.2, activation='relu', batch_norm=True,\n",
    "                 learning_rate=0.001, weight_decay=0.001, device=None):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_dim : int\n",
    "            Number of input features\n",
    "        hidden_layers : list of int\n",
    "            List containing the number of neurons in each hidden layer\n",
    "        output_classes : int\n",
    "            Number of output classes (3 for SPOILED, HALF, FRESH)\n",
    "        dropout_rate : float\n",
    "            Dropout rate for regularization\n",
    "        activation : str\n",
    "            Activation function ('relu', 'sigmoid', 'tanh')\n",
    "        batch_norm : bool\n",
    "            Whether to use batch normalization\n",
    "        learning_rate : float\n",
    "            Learning rate for optimizer\n",
    "        weight_decay : float\n",
    "            Weight decay (L2 regularization) strength\n",
    "        device : str or None\n",
    "            Device to use ('GPU' or 'CPU'). If None, will use GPU if available.\n",
    "        \"\"\"\n",
    "        # Set device\n",
    "        if device is None:\n",
    "            # Let TensorFlow handle device placement\n",
    "            self.device_name = '/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'\n",
    "        else:\n",
    "            self.device_name = f\"/{device.upper()}:0\"\n",
    "        \n",
    "        # Create strategy for device placement\n",
    "        self.strategy = tf.distribute.OneDeviceStrategy(device=self.device_name)\n",
    "        \n",
    "        with self.strategy.scope():\n",
    "            # Initialize model\n",
    "            self.model = self._build_model(\n",
    "                input_dim=input_dim,\n",
    "                hidden_layers=hidden_layers,\n",
    "                output_classes=output_classes,\n",
    "                dropout_rate=dropout_rate,\n",
    "                activation=activation,\n",
    "                batch_norm=batch_norm\n",
    "            )\n",
    "            \n",
    "            # Initialize optimizer with weight decay (L2 regularization)\n",
    "            self.optimizer = optimizers.Adam(\n",
    "                learning_rate=learning_rate, \n",
    "                weight_decay=weight_decay\n",
    "            )\n",
    "            \n",
    "            # Compile the model\n",
    "            self.model.compile(\n",
    "                optimizer=self.optimizer,\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "        \n",
    "        # For tracking training progress\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "        \n",
    "        # For storing the best model\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_model_weights = None\n",
    "        \n",
    "        # For feature scaling\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    def _build_model(self, input_dim, hidden_layers, output_classes, \n",
    "                   dropout_rate, activation, batch_norm):\n",
    "        \"\"\"Build and return a Keras Sequential model.\"\"\"\n",
    "        model = models.Sequential()\n",
    "        \n",
    "        # Input layer\n",
    "        model.add(layers.Input(shape=(input_dim,)))\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for units in hidden_layers:\n",
    "            # Add dense layer\n",
    "            model.add(layers.Dense(units))\n",
    "            \n",
    "            # Add batch normalization if enabled\n",
    "            if batch_norm:\n",
    "                model.add(layers.BatchNormalization())\n",
    "            \n",
    "            # Add activation function\n",
    "            model.add(layers.Activation(activation))\n",
    "            \n",
    "            # Add dropout if enabled\n",
    "            if dropout_rate > 0:\n",
    "                model.add(layers.Dropout(dropout_rate))\n",
    "        \n",
    "        # Add output layer\n",
    "        model.add(layers.Dense(output_classes))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train(self, features, labels, test_size=0.2, batch_size=32, epochs=100, \n",
    "              early_stopping_patience=20, random_state=42, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the neural network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        features : numpy array\n",
    "            Feature matrix\n",
    "        labels : numpy array\n",
    "            Labels\n",
    "        test_size : float\n",
    "            Proportion of data to use for validation\n",
    "        batch_size : int\n",
    "            Batch size for training\n",
    "        epochs : int\n",
    "            Number of training epochs\n",
    "        early_stopping_patience : int\n",
    "            Number of epochs with no improvement after which training will stop\n",
    "        random_state : int\n",
    "            Random seed for reproducibility\n",
    "        verbose : bool\n",
    "            Whether to print training progress\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : MeatClassifier\n",
    "            The trained classifier\n",
    "        \"\"\"\n",
    "        # Convert to numpy arrays if not already\n",
    "        features = np.array(features)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        # Split data into train and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            features, labels, test_size=test_size, \n",
    "            random_state=random_state, stratify=labels\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        X_train = self.scaler.fit_transform(X_train)\n",
    "        X_val = self.scaler.transform(X_val)\n",
    "        \n",
    "        # Define callbacks\n",
    "        callbacks = []\n",
    "        \n",
    "        # Early stopping callback\n",
    "        if early_stopping_patience > 0:\n",
    "            early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=early_stopping_patience,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1 if verbose else 0\n",
    "            )\n",
    "            callbacks.append(early_stopping)\n",
    "        \n",
    "        # Custom callback to store training history\n",
    "        class TrainingHistoryCallback(tf.keras.callbacks.Callback):\n",
    "            def __init__(self, classifier):\n",
    "                super(TrainingHistoryCallback, self).__init__()\n",
    "                self.classifier = classifier\n",
    "                \n",
    "            def on_epoch_end(self, epoch, logs=None):\n",
    "                self.classifier.train_losses.append(logs.get('loss'))\n",
    "                self.classifier.val_losses.append(logs.get('val_loss'))\n",
    "                self.classifier.train_accuracies.append(logs.get('accuracy'))\n",
    "                self.classifier.val_accuracies.append(logs.get('val_accuracy'))\n",
    "                \n",
    "                # Save best model weights\n",
    "                if logs.get('val_loss') < self.classifier.best_val_loss:\n",
    "                    self.classifier.best_val_loss = logs.get('val_loss')\n",
    "                    self.classifier.best_model_weights = self.model.get_weights()\n",
    "        \n",
    "        # Add custom callback\n",
    "        history_callback = TrainingHistoryCallback(self)\n",
    "        callbacks.append(history_callback)\n",
    "        \n",
    "        # Train the model\n",
    "        with self.strategy.scope():\n",
    "            history = self.model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                callbacks=callbacks,\n",
    "                verbose=1 if verbose else 0\n",
    "            )\n",
    "        \n",
    "        # Restore best model weights if available\n",
    "        if self.best_model_weights is not None:\n",
    "            self.model.set_weights(self.best_model_weights)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, features):\n",
    "        \"\"\"\n",
    "        Make predictions on new data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        features : numpy array\n",
    "            Feature matrix\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        numpy array\n",
    "            Predicted class indices\n",
    "        \"\"\"\n",
    "        # Scale features\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        \n",
    "        # Make predictions (logits)\n",
    "        logits = self.model.predict(features_scaled)\n",
    "        \n",
    "        # Get class predictions\n",
    "        predictions = np.argmax(logits, axis=1)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, features):\n",
    "        \"\"\"\n",
    "        Predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        features : numpy array\n",
    "            Feature matrix\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        numpy array\n",
    "            Class probabilities\n",
    "        \"\"\"\n",
    "        # Scale features\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        \n",
    "        # Get logits\n",
    "        logits = self.model.predict(features_scaled)\n",
    "        \n",
    "        # Convert to probabilities using softmax\n",
    "        probabilities = tf.nn.softmax(logits).numpy()\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def evaluate(self, features, labels):\n",
    "        \"\"\"\n",
    "        Evaluate the model on test data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        features : numpy array\n",
    "            Feature matrix\n",
    "        labels : numpy array\n",
    "            True labels\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary containing evaluation metrics\n",
    "        \"\"\"\n",
    "        # Get predictions\n",
    "        y_pred = self.predict(features)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(labels, y_pred)\n",
    "        \n",
    "        # Generate classification report\n",
    "        class_names = ['SPOILED', 'HALF', 'FRESH']\n",
    "        report = classification_report(labels, y_pred, target_names=class_names, output_dict=True)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'report': report\n",
    "        }\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot the training history (loss and accuracy curves).\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Loss plot\n",
    "        ax1.plot(self.train_losses, label='Training Loss')\n",
    "        ax1.plot(self.val_losses, label='Validation Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Loss Curves')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracy plot\n",
    "        ax2.plot(self.train_accuracies, label='Training Accuracy')\n",
    "        ax2.plot(self.val_accuracies, label='Validation Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.set_title('Accuracy Curves')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the model to a file.\"\"\"\n",
    "        # Save Keras model\n",
    "        self.model.save(filepath)\n",
    "        \n",
    "        # Save scaler and other metadata\n",
    "        np.savez(\n",
    "            filepath + '_metadata.npz',\n",
    "            best_val_loss=self.best_val_loss,\n",
    "            scaler_mean=self.scaler.mean_,\n",
    "            scaler_scale=self.scaler.scale_,\n",
    "            scaler_var=self.scaler.var_\n",
    "        )\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load the model from a file.\"\"\"\n",
    "        # Load Keras model\n",
    "        self.model = tf.keras.models.load_model(filepath)\n",
    "        \n",
    "        # Load metadata\n",
    "        metadata = np.load(filepath + '_metadata.npz', allow_pickle=True)\n",
    "        self.best_val_loss = float(metadata['best_val_loss'])\n",
    "        \n",
    "        # Recreate scaler\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.mean_ = metadata['scaler_mean']\n",
    "        self.scaler.scale_ = metadata['scaler_scale']\n",
    "        self.scaler.var_ = metadata['scaler_var']\n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "def train_meat_classifier_tensorflow(features, labels, hidden_layers=[128, 64], \n",
    "                                   learning_rate=0.001, weight_decay=0.001,\n",
    "                                   batch_size=32, epochs=100, test_size=0.2,\n",
    "                                   early_stopping_patience=20, dropout_rate=0.2,\n",
    "                                   activation='relu', batch_norm=True,\n",
    "                                   random_state=42, verbose=True):\n",
    "    \"\"\"\n",
    "    Train and evaluate a neural network for meat classification using TensorFlow.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    features : numpy array\n",
    "        Feature matrix (color histogram, HOG, or LBP features)\n",
    "    labels : numpy array\n",
    "        Labels (0: SPOILED, 1: HALF, 2: FRESH)\n",
    "    hidden_layers : list of int\n",
    "        List of hidden layer sizes\n",
    "    learning_rate : float\n",
    "        Learning rate for optimizer\n",
    "    weight_decay : float\n",
    "        Weight decay (L2 regularization) strength\n",
    "    batch_size : int\n",
    "        Batch size for training\n",
    "    epochs : int\n",
    "        Maximum number of epochs for training\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    early_stopping_patience : int\n",
    "        Number of epochs with no improvement after which training will stop\n",
    "    dropout_rate : float\n",
    "        Dropout rate for regularization\n",
    "    activation : str\n",
    "        Activation function ('relu', 'sigmoid', 'tanh')\n",
    "    batch_norm : bool\n",
    "        Whether to use batch normalization\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    verbose : bool\n",
    "        Whether to print training progress\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing the classifier, evaluation metrics, and other info\n",
    "    \"\"\"\n",
    "    # Set random seeds for reproducibility\n",
    "    tf.random.set_seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Create classifier\n",
    "    classifier = MeatClassifier(\n",
    "        input_dim=features.shape[1],\n",
    "        hidden_layers=hidden_layers,\n",
    "        output_classes=len(np.unique(labels)),\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=activation,\n",
    "        batch_norm=batch_norm,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    # Train the classifier\n",
    "    classifier.train(\n",
    "        features=features,\n",
    "        labels=labels,\n",
    "        test_size=test_size,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        early_stopping_patience=early_stopping_patience,\n",
    "        random_state=random_state,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Split data for evaluation (using the same random state for consistency)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, labels, test_size=test_size, \n",
    "        random_state=random_state, stratify=labels\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    evaluation = classifier.evaluate(X_test, y_test)\n",
    "    \n",
    "    # Plot training history\n",
    "    classifier.plot_training_history()\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'classifier': classifier,\n",
    "        'accuracy': evaluation['accuracy'],\n",
    "        'report': evaluation['report']\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# For color histogram features\n",
    "color_hist_results = train_meat_classifier_tensorflow(\n",
    "    features=np.array(train_features),  # Assuming this is your color histogram features\n",
    "    labels=np.array(train_y),\n",
    "    hidden_layers=[128, 64, 32],\n",
    "    epochs=150,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Print accuracy and classification report\n",
    "print(f\"Test accuracy: {color_hist_results['accuracy']:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "for class_name, metrics in color_hist_results['report'].items():\n",
    "    if isinstance(metrics, dict):  # Skip aggregated metrics\n",
    "        print(f\"{class_name}: Precision={metrics['precision']:.2f}, Recall={metrics['recall']:.2f}, F1-score={metrics['f1-score']:.2f}\")\n",
    "\n",
    "# For HOG features\n",
    "hog_results = train_meat_classifier_tensorflow(\n",
    "    features=np.array(train_features_hog),  # Assuming this is your HOG features\n",
    "    labels=np.array(train_y),\n",
    "    hidden_layers=[64, 32],\n",
    "    epochs=150,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc58d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_hist_results = train_meat_classifier_tensorflow(\n",
    "    features=np.array(train_features_hist),  # Assuming this is your color histogram features\n",
    "    labels=np.array(train_y),\n",
    "    hidden_layers=[128, 64, 32],\n",
    "    epochs=150,\n",
    "    batch_size=16\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
