{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d5ea65b",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93164264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f300792",
   "metadata": {},
   "source": [
    "# Import caching utility functions from local module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96500378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload module when changes are made\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import caching_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e080c648",
   "metadata": {},
   "source": [
    "# Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b79d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label_encoding = {\n",
    "    'SPOILED': 0,\n",
    "    'HALF': 1,\n",
    "    'FRESH': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c987e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(file_path, output_x, output_y):\n",
    "    for file_name in os.listdir(file_path):\n",
    "        class_name = file_name.split('-')[0]\n",
    "        if (class_name == '_classes.csv'): continue\n",
    "        img = cv.imread(file_path + file_name).astype('float32')\n",
    "        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "        img = cv.resize(img, (128, 128), interpolation = cv.INTER_AREA)\n",
    "        img /= 255\n",
    "        output_x.append(img)\n",
    "        output_y.append(class_label_encoding[class_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9e803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "test_x = []\n",
    "test_y = []\n",
    "\n",
    "load_images('data/train/', train_x, train_y)\n",
    "load_images('data/valid/', test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923c8080",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83678282",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bdb528",
   "metadata": {},
   "source": [
    "## Color Histogram\n",
    "Jason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963fdbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_color_histogram(image, bins=32):\n",
    "    \"\"\"\n",
    "    Extract color histogram features from an image.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: The input image (should be in RGB format)\n",
    "    - bins: Number of bins for the histogram\n",
    "    \n",
    "    Returns:\n",
    "    - histogram_features: Flattened histogram features\n",
    "    \"\"\"\n",
    "    # Extract histograms for each channel\n",
    "    hist_r = cv.calcHist([image], [0], None, [bins], [0, 1])  # Changed range to [0, 1] since you're normalizing images\n",
    "    hist_g = cv.calcHist([image], [1], None, [bins], [0, 1])\n",
    "    hist_b = cv.calcHist([image], [2], None, [bins], [0, 1])\n",
    "    \n",
    "    # Normalize the histograms\n",
    "    cv.normalize(hist_r, hist_r, 0, 1, cv.NORM_MINMAX)\n",
    "    cv.normalize(hist_g, hist_g, 0, 1, cv.NORM_MINMAX)\n",
    "    cv.normalize(hist_b, hist_b, 0, 1, cv.NORM_MINMAX)\n",
    "    \n",
    "    # Flatten and concatenate the histograms\n",
    "    histogram_features = np.concatenate([\n",
    "        hist_r.flatten(), \n",
    "        hist_g.flatten(), \n",
    "        hist_b.flatten()\n",
    "    ])\n",
    "    \n",
    "    return histogram_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473f3d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_color_histogram(image, bins=32, title=\"Color Histogram\"):\n",
    "    \"\"\"\n",
    "    Plot the color histogram of an image.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: The input image (should be in RGB format)\n",
    "    - bins: Number of bins for the histogram\n",
    "    - title: Title for the plot\n",
    "    \n",
    "    Returns:\n",
    "    - None (displays the plot)\n",
    "    \"\"\"\n",
    "    # Create a figure with subplots\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    # Display the original image\n",
    "    ax[0].imshow(image)\n",
    "    ax[0].set_title('Original Image')\n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    # Get histogram features using your existing function\n",
    "    features = extract_color_histogram(image, bins)\n",
    "    \n",
    "    # Split the features back into channels\n",
    "    channel_length = len(features) // 3\n",
    "    hist_r = features[:channel_length].reshape(bins, 1)\n",
    "    hist_g = features[channel_length:2*channel_length].reshape(bins, 1)\n",
    "    hist_b = features[2*channel_length:].reshape(bins, 1)\n",
    "    \n",
    "    # Define colors and channels\n",
    "    colors = ['r', 'g', 'b']\n",
    "    channels = ['Red', 'Green', 'Blue']\n",
    "    hists = [hist_r, hist_g, hist_b]\n",
    "    \n",
    "    # Plot histograms for each channel\n",
    "    for i, (hist, col, chan) in enumerate(zip(hists, colors, channels)):\n",
    "        ax[i+1].plot(hist, color=col)\n",
    "        ax[i+1].set_xlim([0, bins])\n",
    "        ax[i+1].set_title(f'{chan} Histogram')\n",
    "        ax[i+1].set_xlabel('Bins')\n",
    "        ax[i+1].set_ylabel('# of Pixels')\n",
    "        ax[i+1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8106ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = train_x[0]  # Get the first image\n",
    "plot_color_histogram(image, bins=32, title=\"Meat Sample Color Histogram\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff30257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_hist(use_cached=True) -> tuple[np.ndarray, np.ndarray]:\n",
    "    train, success = caching_utils.attempt_load_feature_from_cache(\"raw_hist_train.csv\")\n",
    "    if not success or not use_cached: # Not present in cache, regenerate.\n",
    "        train = []\n",
    "        for img in train_x:\n",
    "            hist_features = extract_color_histogram(img)\n",
    "            train.append(hist_features)\n",
    "        train = np.array(train)\n",
    "        caching_utils.save_feature_to_cache(\"raw_hist_train.csv\", train)\n",
    "\n",
    "    test, success = caching_utils.attempt_load_feature_from_cache(\"raw_hist_test.csv\")\n",
    "    if not success or not use_cached:\n",
    "        test = []\n",
    "        for img in test_x:\n",
    "            hist_features = extract_color_histogram(img)\n",
    "            test.append(hist_features)\n",
    "        test = np.array(test)\n",
    "        caching_utils.save_feature_to_cache(\"raw_hist_test.csv\", test)\n",
    "    return (train, test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9a1f32",
   "metadata": {},
   "source": [
    "## Local Binary Pattern\n",
    "Aiden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e319749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel(img, center, x, y): \n",
    "    \"\"\"\n",
    "    Gets Local Binary Patterns values for pixel adjacent to the selected one.\n",
    "    \n",
    "    Parameters:\n",
    "    - img: The image containing the pixels\n",
    "    - center: The pixel Local Binary Patterns is being applied to\n",
    "    - x: The x coordinate of the adjacent pixel\n",
    "    - y: The y coordinate of the adjacent pixel\n",
    "    \n",
    "    Returns:\n",
    "    - new_val: The output value for the comparison between the center pixel and the adjacent pixel\n",
    "    \"\"\"\n",
    "      \n",
    "    new_value = 0\n",
    "      \n",
    "    try: \n",
    "        # if local neighbourhood pixel value is greater than or equal to center pixel values then set it to 1 \n",
    "        if img[x][y] >= center: \n",
    "            new_value = 1\n",
    "              \n",
    "    except: \n",
    "        # exception required when neighbourhood value of center pixel value is null\n",
    "        pass\n",
    "      \n",
    "    return new_value \n",
    "   \n",
    "# Function for calculating LBP \n",
    "def lbp_calculated_pixel(img, x, y): \n",
    "    \"\"\"\n",
    "    Apply the Local Binary Patterns to a single pixel.\n",
    "    \n",
    "    Parameters:\n",
    "    - img: The image containing the pixel\n",
    "    - x: The x coordinate of the selected pixel\n",
    "    - y: The y coordinate of the selected pixel\n",
    "    \n",
    "    Returns:\n",
    "    - val: The output value for the selected pixel after applying Local Binary Patterns\n",
    "    \"\"\"\n",
    "   \n",
    "    center = img[x][y] \n",
    "   \n",
    "    val_ar = [] \n",
    "      \n",
    "    # top_left \n",
    "    val_ar.append(get_pixel(img, center, x-1, y-1)) \n",
    "      \n",
    "    # top \n",
    "    val_ar.append(get_pixel(img, center, x-1, y)) \n",
    "      \n",
    "    # top_right \n",
    "    val_ar.append(get_pixel(img, center, x-1, y + 1)) \n",
    "      \n",
    "    # right \n",
    "    val_ar.append(get_pixel(img, center, x, y + 1)) \n",
    "      \n",
    "    # bottom_right \n",
    "    val_ar.append(get_pixel(img, center, x + 1, y + 1)) \n",
    "      \n",
    "    # bottom \n",
    "    val_ar.append(get_pixel(img, center, x + 1, y)) \n",
    "      \n",
    "    # bottom_left \n",
    "    val_ar.append(get_pixel(img, center, x + 1, y-1)) \n",
    "      \n",
    "    # left \n",
    "    val_ar.append(get_pixel(img, center, x, y-1)) \n",
    "       \n",
    "    # convert binary values to decimal \n",
    "    power_val = [1, 2, 4, 8, 16, 32, 64, 128] \n",
    "   \n",
    "    val = 0\n",
    "      \n",
    "    for i in range(len(val_ar)): \n",
    "        val += val_ar[i] * power_val[i] \n",
    "          \n",
    "    return val\n",
    "\n",
    "\n",
    "def lbp_output(img_bgr):\n",
    "    \"\"\"\n",
    "    Apply the Local Binary Patterns filter to an image.\n",
    "    \n",
    "    Parameters:\n",
    "    - img_bgr: The image to be transformed\n",
    "    \n",
    "    Returns:\n",
    "    - image_lbp: The output image with the Local Binary Patterns filter applied\n",
    "    \"\"\"\n",
    "    height, width, _ = img_bgr.shape \n",
    "   \n",
    "    # convert RGB to gray \n",
    "    img_gray = cv.cvtColor(img_bgr, \n",
    "                            cv.COLOR_BGR2GRAY) \n",
    "       \n",
    "    # create numpy array as same height and width of RGB image \n",
    "    img_lbp = np.zeros((height, width), \n",
    "                       np.float32) \n",
    "       \n",
    "    for i in range(0, height): \n",
    "        for j in range(0, width): \n",
    "            img_lbp[i, j] = lbp_calculated_pixel(img_gray, i, j)\n",
    "\n",
    "    return img_lbp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b0012",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_bgr = train_x[0]\n",
    "img_lbp = lbp_output(img_bgr)\n",
    "  \n",
    "plt.imshow(img_bgr) \n",
    "plt.show()\n",
    "   \n",
    "plt.imshow(img_lbp, cmap =\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc8defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images_lbp(imgs, labels, train_test='train'):\n",
    "    \"\"\"\n",
    "    Save images with the Local Binary Patterns filter applied and unique file names based on label.\n",
    "    \n",
    "    Parameters:\n",
    "    - imgs: List of images to be transformed\n",
    "    - labels: List of labels for each image\n",
    "    - train_test: The string value either 'train' or 'test' which determines the output directory\n",
    "    \"\"\"\n",
    "    label_text = ['SPOILED', 'HALF', 'FRESH']\n",
    "    for image in range(len(imgs)):\n",
    "        lbp_image = lbp_output(imgs[image])\n",
    "        filename = f'data/lbp/{train_test}/{label_text[labels[image]]}-{image}-lbp.jpg'\n",
    "        cv.imwrite(filename, lbp_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8bc888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_images_lbp(train_x, train_y, train_test='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b74f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_images_lbp(test_x, test_y, train_test='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905fa998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_lbp(use_cached=True) -> tuple[np.ndarray, np.ndarray]:\n",
    "    train, success = caching_utils.attempt_load_feature_from_cache(\"raw_lbp_train.csv\")\n",
    "    if not success or not use_cached:\n",
    "        train = []\n",
    "        for img in train_x:\n",
    "            lbp_features = lbp_output(img).flatten()\n",
    "            train.append(lbp_features)\n",
    "        train = np.array(train)\n",
    "        caching_utils.save_feature_to_cache(\"raw_lbp_train.csv\", train)\n",
    "    \n",
    "    test, success = caching_utils.attempt_load_feature_from_cache(\"raw_lbp_test.csv\")\n",
    "    if not success or not use_cached:\n",
    "        test = []\n",
    "        for img in test_x:\n",
    "            lbp_features = lbp_output(img).flatten()\n",
    "            test.append(lbp_features)\n",
    "        test = np.array(test)\n",
    "        caching_utils.save_feature_to_cache(\"raw_lbp_test.csv\", test)\n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8f85ec",
   "metadata": {},
   "source": [
    "## Histograms of Oriented Gradients\n",
    "Fiona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e97ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from here: https://scikit-image.org/docs/stable/auto_examples/features_detection/plot_hog.html\n",
    "def make_hog(image, visualize=False):\n",
    "    features, hog_image = hog(\n",
    "            image,\n",
    "            orientations=16,\n",
    "            pixels_per_cell=(8, 8),\n",
    "            cells_per_block=(1, 1),\n",
    "            visualize=True,\n",
    "            channel_axis=-1\n",
    "        )\n",
    "\n",
    "    if visualize:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "\n",
    "        ax1.axis('off')\n",
    "        ax1.imshow(image, cmap=plt.cm.gray)\n",
    "        ax1.set_title('Input image')\n",
    "\n",
    "        hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "\n",
    "        ax2.axis('off')\n",
    "        ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
    "        ax2.set_title('Histogram of Oriented Gradients')\n",
    "        plt.show()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a679f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_hog(train_x[1], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3f25c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_hog(use_cached=True) -> tuple[np.ndarray, np.ndarray]:\n",
    "    train, success = caching_utils.attempt_load_feature_from_cache(\"raw_hog_train.csv\")\n",
    "    if not success or not use_cached:\n",
    "        train = []\n",
    "        for img in train_x:\n",
    "            hog_features = make_hog(img)\n",
    "            train.append(hog_features)\n",
    "        train = np.array(train)\n",
    "        caching_utils.save_feature_to_cache(\"raw_hog_train.csv\", train)\n",
    "\n",
    "    test, success = caching_utils.attempt_load_feature_from_cache(\"raw_hog_test.csv\")\n",
    "    if not success or not use_cached:\n",
    "        test = []\n",
    "        for img in test_x:\n",
    "            hog_features = make_hog(img)\n",
    "            test.append(hog_features)\n",
    "        test = np.array(test)\n",
    "        caching_utils.save_feature_to_cache(\"raw_hog_test.csv\", test)\n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af89b99",
   "metadata": {},
   "source": [
    "## Load extracted features from cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf368aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_hist, test_features_hist = load_feature_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9d4b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_lbp, test_features_lbp = load_feature_lbp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4948bc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_hog, test_features_hog = load_feature_hog()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d902495e",
   "metadata": {},
   "source": [
    "# Convert features to dataframe and apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cd80fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_features_to_dataframe(train_features, train_labels, test_features, test_labels,\n",
    "                                    pca_components=2) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Transform a set of extracted features into a pandas DataFrame and applies PCA.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_features: List of extracted features for training (numpy arrays)\n",
    "    - train_labels: Labels for training\n",
    "    - test_features: List of extracted features for training (numpy arrays)\n",
    "    - test_labels: Labels for testing\n",
    "    - pca_components: Number of PCA components to keep (default=2)\n",
    "    - labels: Optional list of labels for the images\n",
    "    \n",
    "    Returns:\n",
    "    - tuple (train_dataframe, test_dataframe)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=pca_components)\n",
    "    pca.fit(train_features)\n",
    "\n",
    "    train_features_pca = pca.transform(train_features)\n",
    "    test_features_pca = pca.transform(test_features)\n",
    "    \n",
    "    # Create DataFrame with PCA results\n",
    "    columns = [f'pca_{i+1}' for i in range(pca_components)]\n",
    "    train_df = pd.DataFrame(train_features_pca, columns=columns)\n",
    "    train_df['label'] = train_labels\n",
    "\n",
    "    test_df = pd.DataFrame(test_features_pca, columns=columns)\n",
    "    test_df['label'] = test_labels\n",
    "    \n",
    "    return (train_df, test_df)\n",
    "\n",
    "\"\"\"\n",
    "Example usage:\n",
    "train, test = transform_features_to_dataframe(x_train, y_train, x_test, y_test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0be869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframe(train_features: np.ndarray, train_labels: np.ndarray,\n",
    "                   test_features: np.ndarray, test_labels: np.ndarray,\n",
    "                   name: str, pca_components: int=2, use_cached=True) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    target = f\"{name}_pca_{pca_components}\"\n",
    "\n",
    "    train, train_success = caching_utils.attempt_load_dataframe(target + \"_train\")\n",
    "    test, test_success = caching_utils.attempt_load_dataframe(target + \"_test\")\n",
    "\n",
    "    if not train_success or not test_success or not use_cached:\n",
    "        train, test = transform_features_to_dataframe(train_features, train_labels, test_features, test_labels, pca_components=pca_components)\n",
    "        caching_utils.save_dataframe_to_cache(target + \"_train\", train)\n",
    "        caching_utils.save_dataframe_to_cache(target + \"_test\", test)\n",
    "    \n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1867f88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df[df.drop(['label'], axis=1).columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70d7251",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_hist, test_df_hist = make_dataframe(train_features_hist, train_y, test_features_hist, test_y, \"hist\", pca_components=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc83eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_lbp, test_df_lbp = make_dataframe(train_features_lbp, train_y, test_features_lbp, test_y, \"lbp\", pca_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4d5225",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_hog, test_df_hog = make_dataframe(train_features_hog, train_y, test_features_hog, test_y, \"hog\", pca_components=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea924b22",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651a35fb",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "Jason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe5d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_tree(x_train_tree, y_train_tree, x_test_tree, y_test_tree, max_depth=5, \n",
    "                   min_samples_split=2, criterion='gini', show_tree=True, feature_names=None):\n",
    "    \"\"\"\n",
    "    Train a decision tree classifier on any type of features, with optional histogram visualization.\n",
    "    \n",
    "    Parameters:\n",
    "    - x_train_tree: Training features\n",
    "    - y_train_tree: Training labels\n",
    "    - x_test_tree: Test features\n",
    "    - y_test_tree: Test labels\n",
    "    - max_depth: Maximum depth of the decision tree\n",
    "    - min_samples_split: The minimum number of samples required to split an internal node\n",
    "    - criterion: The function to measure the quality of a split ('gini' or 'entropy')\n",
    "    - show_tree: Whether to visualize the decision tree\n",
    "    - feature_names: Names of features (will be auto-generated if None)\n",
    "    \n",
    "    Returns:\n",
    "    - dt_classifier: Trained decision tree classifier\n",
    "    - accuracy: Classification accuracy on test set\n",
    "    - report: Classification report\n",
    "    - conf_matrix: Confusion matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a list of class names\n",
    "    class_names=['SPOILED', 'HALF', 'FRESH']\n",
    "    \n",
    "    # Create and train a Decision Tree classifier\n",
    "    dt_classifier = DecisionTreeClassifier(max_depth=max_depth, \n",
    "                                          min_samples_split=min_samples_split,\n",
    "                                          criterion=criterion,\n",
    "                                          random_state=42)\n",
    "    dt_classifier.fit(x_train_tree, y_train_tree)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = dt_classifier.predict(x_test_tree)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test_tree, predictions)\n",
    "    report = classification_report(y_test_tree, predictions, target_names=class_names)\n",
    "    conf_matrix = confusion_matrix(y_test_tree, predictions)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Decision Tree Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Criterion:\", criterion)\n",
    "    print(\"Min Samples Split:\", min_samples_split)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Display confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    \n",
    "    # Add text annotations in the confusion matrix\n",
    "    thresh = conf_matrix.max() / 2\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        for j in range(conf_matrix.shape[1]):\n",
    "            plt.text(j, i, format(conf_matrix[i, j], 'd'),\n",
    "                    horizontalalignment=\"center\",\n",
    "                    color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "    # Show decision tree if requested\n",
    "    if show_tree:\n",
    "        # Create feature names if not provided\n",
    "        if feature_names is None:\n",
    "            feature_names = [f\"Feature_{i}\" for i in range(x_train_tree.shape[1])]\n",
    "            \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plot_tree(dt_classifier, \n",
    "                  feature_names=feature_names,\n",
    "                  class_names=class_names,\n",
    "                  filled=True, \n",
    "                  rounded=True, \n",
    "                  fontsize=8)\n",
    "        plt.title(f\"Decision Tree (criterion={criterion}, min_samples_split={min_samples_split})\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return dt_classifier, accuracy, report, conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfb48d5",
   "metadata": {},
   "source": [
    "## Decision Tree Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6ab4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_decision_tree_on_pca_dataframes(train_dfs, test_dfs, train_labels, test_labels, \n",
    "                              feature_names=None, max_depth=3, \n",
    "                              min_samples_split=2, criterion='gini'):\n",
    "    \"\"\"\n",
    "    Run decision tree classification on multiple PCA-transformed dataframes.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_dfs: Dictionary of training dataframes {'name': dataframe}\n",
    "    - test_dfs: Dictionary of testing dataframes {'name': dataframe}\n",
    "    - train_labels: Training labels\n",
    "    - test_labels: Testing labels\n",
    "    - feature_names: Optional dictionary of feature names for each dataframe\n",
    "    - max_depth: Maximum depth for decision tree\n",
    "    - min_samples_split: The minimum number of samples required to split an internal node\n",
    "    - criterion: The function to measure the quality of a split ('gini' or 'entropy')\n",
    "    \n",
    "    Returns:\n",
    "    - results: Dictionary containing trained models and their performance metrics\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for name in train_dfs.keys():\n",
    "        print(f\"\\n--- Decision Tree Classification for {name} features ---\")\n",
    "        print(f\"Parameters: max_depth={max_depth}, min_samples_split={min_samples_split}, criterion={criterion}\")\n",
    "        \n",
    "        # Get the dataframes\n",
    "        train_df = train_dfs[name]\n",
    "        test_df = test_dfs[name]\n",
    "        \n",
    "        # Extract features without the label column\n",
    "        if 'label' in train_df.columns:\n",
    "            train_features = train_df.drop('label', axis=1).values\n",
    "            test_features = test_df.drop('label', axis=1).values\n",
    "        else:\n",
    "            train_features = train_df.values\n",
    "            test_features = test_df.values\n",
    "            \n",
    "        # Determine feature names if not provided\n",
    "        names = None\n",
    "        if feature_names and name in feature_names:\n",
    "            names = feature_names[name]\n",
    "        else:\n",
    "            if 'label' in train_df.columns:\n",
    "                names = train_df.drop('label', axis=1).columns.tolist()\n",
    "            else:\n",
    "                names = train_df.columns.tolist()\n",
    "        \n",
    "        # Train decision tree\n",
    "        model, accuracy, report, conf_matrix = train_decision_tree(\n",
    "            x_train_tree=train_features,\n",
    "            y_train_tree=train_labels,\n",
    "            x_test_tree=test_features,\n",
    "            y_test_tree=test_labels,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            criterion=criterion,\n",
    "            show_tree=True,\n",
    "            feature_names=names\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'accuracy': accuracy,\n",
    "            'report': report,\n",
    "            'confusion_matrix': conf_matrix\n",
    "        }\n",
    "    \n",
    "    # Compare results\n",
    "    print(\"\\n--- Comparison of Decision Tree Results ---\")\n",
    "    for name, result in results.items():\n",
    "        print(f\"{name}: Accuracy = {result['accuracy']:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b95b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parameter_performance(parameter_values, accuracies, parameter_name, criteria=None):\n",
    "    \"\"\"\n",
    "    Plot the performance of a decision tree parameter.\n",
    "    \n",
    "    Parameters:\n",
    "    - parameter_values: List of parameter values tested\n",
    "    - accuracies: List of lists containing accuracy values for each parameter value\n",
    "    - parameter_name: Name of the parameter being tested\n",
    "    - criteria: Optional list of criterion names if testing both gini and entropy\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    if criteria:\n",
    "        # We're plotting multiple lines (one for each criterion)\n",
    "        for i, criterion in enumerate(criteria):\n",
    "            plt.plot(parameter_values, accuracies[i], marker='o', label=f'Criterion: {criterion}')\n",
    "    else:\n",
    "        # Just one line for a single criterion\n",
    "        plt.plot(parameter_values, accuracies, marker='o')\n",
    "    \n",
    "    plt.xlabel(parameter_name)\n",
    "    plt.ylabel('Test Accuracy')\n",
    "    plt.title(f'Decision Tree Performance vs {parameter_name}')\n",
    "    if criteria:\n",
    "        plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fff854",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Aiden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf70816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(x_train_forest, y_train_forest, x_test_forest, y_test_forest, n_estimators=100, criterion='gini', max_depth=None,\n",
    "                        min_samples_split=2, min_samples_leaf=1, max_features='sqrt'):\n",
    "    \"\"\"\n",
    "    Train and generate evaluation metrics for a Random Forest classifier given training and testing data.\n",
    "    \n",
    "    Parameters:\n",
    "    - x_train_forest: Training data\n",
    "    - y_train_forest: Training labels\n",
    "    - x_test_forest: Test data\n",
    "    - y_test_forest: Test labels\n",
    "    - n_estimators: The number of trees in the forest\n",
    "    - criterion: The function to measure the quality of a split\n",
    "    - max_depth: Maximum depth of the tree\n",
    "    - min_samples_split: The minimum number of samples required to split an internal node\n",
    "    - min_samples_leaf: The minimum number of samples required to be at a leaf node\n",
    "    - max_features: The number of features to consider when looking for the best split\n",
    "    \n",
    "    Returns:\n",
    "    - accuracy: The accuracy score for the model's predictions\n",
    "    - precision: The precision score for the model's predictions\n",
    "    - recall: The recall score for the model's predictions\n",
    "    - f1: The f1 score for the model's predictions\n",
    "    - confusion: The confusion matrix fro the model's predictions\n",
    "    \"\"\"\n",
    "    # Create Random Forest classifer object\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split,\n",
    "                                min_samples_leaf=min_samples_leaf, max_features=max_features, random_state=42)\n",
    "    \n",
    "    # Train Random Forest Classifer\n",
    "    clf.fit(x_train_forest,y_train_forest)\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(x_test_forest)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_forest, y_pred)\n",
    "    precision = precision_score(y_test_forest, y_pred, average='macro')\n",
    "    recall = recall_score(y_test_forest, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test_forest, y_pred, average='macro')\n",
    "    confusion = confusion_matrix(y_test_forest, y_pred)\n",
    "\n",
    "    return accuracy, precision, recall, f1, confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b08ef2",
   "metadata": {},
   "source": [
    "## Random Forest Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b08df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rf(rf_train_x, rf_train_y, rf_test_x, rf_test_y):\n",
    "    \"\"\"\n",
    "    Run a series of tests with different combinations of hyperparameters for the Random Forest classifier.\n",
    "    \n",
    "    Parameters:\n",
    "    - rf_train_x: Training data\n",
    "    - rf_train_y: Training lables\n",
    "    - rf_test_x: Test data\n",
    "    - rf_test_y: Test labels\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame(all_tests): The dataframe containing all hyperparameter values and results for each test\n",
    "    - confusion_matrices: List of confusion matrices for each test\n",
    "    \"\"\"\n",
    "    # Hyperparameter tuning values\n",
    "    all_n_estimators = [20,100,500]\n",
    "    all_criterion = ['gini', 'entropy', 'log_loss']\n",
    "    all_max_depth = [10, 20, 100]\n",
    "    all_min_samples_split = [2]\n",
    "    all_min_samples_leaf = [1]\n",
    "    all_max_features = ['sqrt','log2',None]\n",
    "\n",
    "    # Run tests with all combinations and output results\n",
    "    all_tests = {'n_estimators': [], 'criterion': [], 'max_depth': [], 'min_samples_split': [], 'min_samples_leaf': [], 'max_features': [],\n",
    "                'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "    confusion_matrices = []\n",
    "    for n_estimators in all_n_estimators:\n",
    "        for criterion in all_criterion:\n",
    "            for max_depth in all_max_depth:\n",
    "                for min_samples_split in all_min_samples_split:\n",
    "                    for min_samples_leaf in all_min_samples_leaf:\n",
    "                        for max_features in all_max_features:\n",
    "                            accuracy, precision, recall, f1, confusion = train_random_forest(rf_train_x, rf_train_y, rf_test_x, rf_test_y, n_estimators,\n",
    "                                                                                            criterion, max_depth, min_samples_split, min_samples_leaf,\n",
    "                                                                                            max_features)\n",
    "                            for i, feature in enumerate(all_tests):\n",
    "                                feature_vals = [n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, max_features,\n",
    "                                               accuracy, precision, recall, f1]\n",
    "                                all_tests[feature].append(feature_vals[i])\n",
    "                            confusion_matrices.append(confusion)\n",
    "\n",
    "    return pd.DataFrame(all_tests), confusion_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b19296f-4c48-46d3-bdf0-e33f62d12f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_hist, test_df_hist = make_dataframe(train_features_hist, train_y, test_features_hist, test_y, \"hist\", pca_components=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a53dacf-0987-457c-aed2-45004dc60b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_lbp, test_df_lbp = make_dataframe(train_features_lbp, train_y, test_features_lbp, test_y, \"lbp\", pca_components=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22d06b0-988e-4014-92d5-9bd6cf51a0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_hog, test_df_hog = make_dataframe(train_features_hog, train_y, test_features_hog, test_y, \"hog\", pca_components=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e437ed5a-74bf-4099-856c-e25fe1dd8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_rf = {}\n",
    "test_x_rf = {}\n",
    "train_y_rf = {}\n",
    "test_y_rf = {}\n",
    "for data in [(train_df_hist, 'train', 'hist'), (test_df_hist, 'test', 'hist'), (train_df_lbp, 'train', 'lbp'), (test_df_lbp, 'test', 'lbp'),\n",
    "           (train_df_hog, 'train', 'hog'), (test_df_hog, 'test', 'hog')]:\n",
    "    if data[1] == 'train':\n",
    "        train_x_rf[data[2]] = data[0].drop('label', axis=1)\n",
    "        train_y_rf[data[2]] = data[0]['label']\n",
    "    else:\n",
    "        test_x_rf[data[2]] = data[0].drop('label', axis=1)\n",
    "        test_y_rf[data[2]] = data[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd073c-faf8-4d67-a473-3910a446f3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for df in ['hist','lbp','hog']:\n",
    "    result_frame, result_conf = test_rf(train_x_rf[df], train_y_rf[df], test_x_rf[df], test_y_rf[df])\n",
    "    results[df] = result_frame\n",
    "    results[f'{df}_conf'] = result_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6cfcd3-1bb3-4a9b-90fc-a990a42b28ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results['hog'].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c24ef9-83b1-473d-b6c3-444dc9dacc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_results = results['hog_conf'][57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8da7df-93a1-418b-b9f8-539ec8bb7a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_display = ConfusionMatrixDisplay(confusion_matrix = matrix_results, display_labels = ['SPOILED', 'HALF', 'FRESH'])\n",
    "\n",
    "confusion_matrix_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b46b2df",
   "metadata": {},
   "source": [
    "## Knn\n",
    "Fiona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f653c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I Liked Jason's implementation of the docstring and report visualization in train_decision_tree\n",
    "# so I modified my existing implementation to work similarly.\n",
    "def train_knn(x_train: pd.DataFrame, y_train: pd.DataFrame, x_test: pd.DataFrame, y_test: pd.DataFrame,\n",
    "              n_neighbors=5, weights='uniform', p=2, report=False) -> tuple[\n",
    "                  KNeighborsClassifier, float, float, float, float,\n",
    "                  np.ndarray]:\n",
    "    \"\"\"\n",
    "    Train and generate evaluation metrics for a Knn classifier.\n",
    "    All returned metrics are calculated using the macro method.\n",
    "    \n",
    "    Parameters:\n",
    "    - x_train: Training features.\n",
    "    - y_train: Training labels.\n",
    "    - x_test: Testing features.\n",
    "    - y_test: Testing labels.\n",
    "\n",
    "    - n_neighbors: int - Number of neighbors to use, default 5.\n",
    "    - weights: 'uniform' | 'distance' - Weight function to use, default 'uniform'.\n",
    "    - p: int - Power parameter, default 2.\n",
    "\n",
    "    - report: bool - Visualize report, default False.\n",
    "\n",
    "    Returns:\n",
    "    - knn: Trained Knn classifier.\n",
    "    - accuracy: Classification accuracy.\n",
    "    - precision: Classifier precision.\n",
    "    - recall: Model recall.\n",
    "    - f1-score: Model F1-score.\n",
    "    - conf_matrix: Confusion matrix.\n",
    "    \"\"\"\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, p=p)\n",
    "    knn.fit(x_train, y_train)\n",
    "\n",
    "    pred = knn.predict(x_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred, average='macro')\n",
    "    recall = recall_score(y_test, pred, average='macro')\n",
    "    f1 = f1_score(y_test, pred, average='macro')\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, pred)\n",
    "\n",
    "    if report:\n",
    "        class_names = ['SPOILED', 'HALF', 'FRESH']\n",
    "\n",
    "        print(\"Knn Classifier performance:\")\n",
    "        print(f\"N-neighbors: {n_neighbors}\")\n",
    "        print(f\"Weight type: {weights}\")\n",
    "        print(f\"p: {p}\\n\")\n",
    "\n",
    "        print(f\"Accuracy {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Purples)\n",
    "        plt.title('Confusion Matrix (Knn)')\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(class_names))\n",
    "        plt.xticks(tick_marks, class_names, rotation=45)\n",
    "        plt.yticks(tick_marks, class_names)\n",
    "\n",
    "        thresh = conf_matrix.max() / 2\n",
    "        for i in range(conf_matrix.shape[0]):\n",
    "            for j in range(conf_matrix.shape[1]):\n",
    "                plt.text(j, i, format(conf_matrix[i, j], 'd'),\n",
    "                         horizontalalignment=\"center\",\n",
    "                         color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.show()\n",
    "        \n",
    "    return (knn, accuracy, precision, recall, f1, conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf681e06",
   "metadata": {},
   "source": [
    "## Knn Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15e6155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_knn(x_train: pd.DataFrame, y_train: pd.DataFrame,\n",
    "             x_test: pd.DataFrame, y_test: pd.DataFrame) -> tuple[\n",
    "                 pd.DataFrame,\n",
    "                 np.ndarray\n",
    "             ]:\n",
    "    \"\"\"\n",
    "    Test different permutations of hyperparameters for the Knn classifier.\n",
    "\n",
    "    Parameters:\n",
    "    - x_train: Training features.\n",
    "    - y_train: Training labels.\n",
    "    - x_test: Testing features.\n",
    "    - y_test: Testing labels.\n",
    "\n",
    "    Returns:\n",
    "    - all_tests: Dataframe containing all hyperparameter values and their corresponding performance metrics.\n",
    "    - confusion: List of confusion matricies for all tests.\n",
    "    \"\"\"\n",
    "    # Hyperparameter values\n",
    "    all_n_neighbors = [5, 10, 25, 50, 100, 250]\n",
    "    all_weights = ['uniform', 'distance']\n",
    "    all_p = [1, 2, 3, 5, 7, 10]\n",
    "\n",
    "    # Run tests and save results.\n",
    "    all_tests = {'n_neighbors': [], 'weight': [], 'p': [],\n",
    "                 'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "    confusion_matrices = []\n",
    "\n",
    "    for n_neighbors in all_n_neighbors:\n",
    "        for weight in all_weights:\n",
    "            for p in all_p:\n",
    "                all_tests['n_neighbors'].append(n_neighbors)\n",
    "                all_tests['weight'].append(weight)\n",
    "                all_tests['p'].append(p)\n",
    "\n",
    "                knn, accuracy, precision, recall, f1, conf_matrix = train_knn(\n",
    "                    x_train, y_train, x_test, y_test,\n",
    "                    n_neighbors=n_neighbors, weights=weight, p=p\n",
    "                )\n",
    "\n",
    "                all_tests['accuracy'].append(accuracy)\n",
    "                all_tests['precision'].append(precision)\n",
    "                all_tests['recall'].append(recall)\n",
    "                all_tests['f1'].append(f1)\n",
    "                confusion_matrices.append(conf_matrix)\n",
    "    return (pd.DataFrame.from_dict(all_tests), confusion_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b82867",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "Aiden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf7e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(x_train_mlp, y_train_mlp, x_test_mlp, y_test_mlp, hidden_layer_sizes, max_iter=100, activation='relu'):\n",
    "    \"\"\"\n",
    "    Train and generate evaluation metrics for an MLP classifier given training and testing data.\n",
    "    \n",
    "    Parameters:\n",
    "    - x_train_mlp: Training data\n",
    "    - y_train_mlp: Training labels\n",
    "    - x_test_mlp: Test data\n",
    "    - y_test_mlp: Test labels\n",
    "    - hidden_layer_sizes: The size of each hidden layer in tuple form (each entry is a hidden layer)\n",
    "    - max_iter: The number of iterations for tuning weights and biases\n",
    "    - activation: The activation function of the perceptrons\n",
    "    \n",
    "    Returns:\n",
    "    - accuracy: The accuracy score for the model's predictions\n",
    "    - precision: The precision score for the model's predictions\n",
    "    - recall: The recall score for the model's predictions\n",
    "    - f1: The f1 score for the model's predictions\n",
    "    - confusion: The confusion matrix fro the model's predictions\n",
    "    \"\"\"\n",
    "    # Create MLP classifer object\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, max_iter=max_iter, activation=activation, random_state=42)\n",
    "    \n",
    "    # Train MLP Classifer\n",
    "    mlp.fit(x_train_mlp,y_train_mlp)\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = mlp.predict(x_test_mlp)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_mlp, y_pred)\n",
    "    precision = precision_score(y_test_mlp, y_pred, average='macro')\n",
    "    recall = recall_score(y_test_mlp, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test_mlp, y_pred, average='macro')\n",
    "    confusion = confusion_matrix(y_test_mlp, y_pred)\n",
    "\n",
    "    return accuracy, precision, recall, f1, confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f13511",
   "metadata": {},
   "source": [
    "## Neural Network Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e7d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nn(nn_train_x, nn_train_y, nn_test_x, nn_test_y):\n",
    "    \"\"\"\n",
    "    Run a series of tests with different combinations of hyperparameters for the MLP classifier.\n",
    "    \n",
    "    Parameters:\n",
    "    - nn_train_x: Training data\n",
    "    - nn_train_y: Training lables\n",
    "    - nn_test_x: Test data\n",
    "    - nn_test_y: Test labels\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame(all_tests): The dataframe containing all hyperparameter values and results for each test\n",
    "    - confusion_matrices: List of confusion matrices for each test\n",
    "    \"\"\"\n",
    "    # Hyperparameter tuning values\n",
    "    all_hidden_layer_sizes = [(200,100), (200,50,20)]\n",
    "    all_max_iter = [10000]\n",
    "    all_activation = ['relu', 'logistic']\n",
    "\n",
    "    # Run tests with all combinations and output results\n",
    "    all_tests = {'hidden_layer_sizes': [], 'max_iter': [], 'activation': [],\n",
    "                'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "    confusion_matrices = []\n",
    "    for hidden_layer_sizes in all_hidden_layer_sizes:\n",
    "        for max_iter in all_max_iter:\n",
    "            for activation in all_activation:\n",
    "                accuracy, precision, recall, f1, confusion = train_neural_network(nn_train_x, nn_train_y, nn_test_x, nn_test_y, hidden_layer_sizes,\n",
    "                                                                                max_iter, activation)\n",
    "                for i, feature in enumerate(all_tests):\n",
    "                    feature_vals = [hidden_layer_sizes, max_iter, activation, accuracy, precision, recall, f1]\n",
    "                    all_tests[feature].append(feature_vals[i])\n",
    "                confusion_matrices.append(confusion)\n",
    "\n",
    "    return pd.DataFrame(all_tests), confusion_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b77d8a-2b3a-4189-ac72-e5029d235775",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_nn = {}\n",
    "test_x_nn = {}\n",
    "train_y_nn = {}\n",
    "test_y_nn = {}\n",
    "for data in [(train_df_hist, 'train', 'hist'), (test_df_hist, 'test', 'hist'), (train_df_lbp, 'train', 'lbp'), (test_df_lbp, 'test', 'lbp'),\n",
    "           (train_df_hog, 'train', 'hog'), (test_df_hog, 'test', 'hog')]:\n",
    "    if data[1] == 'train':\n",
    "        train_x_nn[data[2]] = data[0].drop('label', axis=1)\n",
    "        train_y_nn[data[2]] = data[0]['label']\n",
    "    else:\n",
    "        test_x_nn[data[2]] = data[0].drop('label', axis=1)\n",
    "        test_y_nn[data[2]] = data[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e33ef7-0529-4c73-a43a-2d5a1874f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_nn = {}\n",
    "for df in ['hist','lbp','hog']:\n",
    "    result_frame, result_conf = test_nn(train_x_nn[df], train_y_nn[df], test_x_nn[df], test_y_nn[df])\n",
    "    results_nn[df] = result_frame\n",
    "    results_nn[f'{df}_conf'] = result_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34e3633-ee4a-4870-bacd-56b4e9231f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_nn['hog'].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02dd61c-b8a1-46d4-ab71-380002e26f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_results = results_nn['hog_conf'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd675d6-13ef-41a9-968d-1b379eab981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_display = ConfusionMatrixDisplay(confusion_matrix = matrix_results, display_labels = ['SPOILED', 'HALF', 'FRESH'])\n",
    "\n",
    "confusion_matrix_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6e05cc",
   "metadata": {},
   "source": [
    "# Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adc9297",
   "metadata": {},
   "source": [
    "## Parameter Tuning for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e7c524",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dfs = {\n",
    "    'Histogram': train_df_hist,\n",
    "    'LBP': train_df_lbp,\n",
    "    'HOG': train_df_hog\n",
    "}\n",
    "\n",
    "test_dfs = {\n",
    "    'Histogram': test_df_hist,\n",
    "    'LBP': test_df_lbp,\n",
    "    'HOG': test_df_hog\n",
    "}\n",
    "\n",
    "# Define feature names for better visualization\n",
    "pca_components = 16\n",
    "feature_names = {\n",
    "    'Histogram': [f'Histogram PCA {i+1}' for i in range(pca_components)],\n",
    "    'LBP': [f'LBP PCA {i+1}' for i in range(pca_components)],\n",
    "    'HOG': [f'HOG PCA {i+1}' for i in range(pca_components)]\n",
    "}\n",
    "\n",
    "# Test different min_samples_split values with Gini criterion\n",
    "print(\"\\n--- Testing different min_samples_split values with Gini criterion ---\")\n",
    "min_samples_splits_to_test = [2, 15, 50]\n",
    "gini_accuracies = []\n",
    "entropy_accuracies = []\n",
    "\n",
    "# First test with Gini\n",
    "for min_samples in min_samples_splits_to_test:\n",
    "    print(f\"\\nTesting min_samples_split = {min_samples} with criterion = 'gini'\")\n",
    "    results = run_decision_tree_on_pca_dataframes(\n",
    "        train_dfs=train_dfs,\n",
    "        test_dfs=test_dfs,\n",
    "        train_labels=train_y,\n",
    "        test_labels=test_y,\n",
    "        feature_names=feature_names,\n",
    "        max_depth=3,\n",
    "        min_samples_split=min_samples,\n",
    "        criterion='gini'\n",
    "    )\n",
    "    # Store average accuracy across all feature types\n",
    "    avg_accuracy = sum(result['accuracy'] for result in results.values()) / len(results)\n",
    "    gini_accuracies.append(avg_accuracy)\n",
    "\n",
    "# Next test with Entropy\n",
    "for min_samples in min_samples_splits_to_test:\n",
    "    print(f\"\\nTesting min_samples_split = {min_samples} with criterion = 'entropy'\")\n",
    "    results = run_decision_tree_on_pca_dataframes(\n",
    "        train_dfs=train_dfs,\n",
    "        test_dfs=test_dfs,\n",
    "        train_labels=train_y,\n",
    "        test_labels=test_y,\n",
    "        feature_names=feature_names,\n",
    "        max_depth=3,\n",
    "        min_samples_split=min_samples,\n",
    "        criterion='entropy'\n",
    "    )\n",
    "    # Store average accuracy across all feature types\n",
    "    avg_accuracy = sum(result['accuracy'] for result in results.values()) / len(results)\n",
    "    entropy_accuracies.append(avg_accuracy)\n",
    "\n",
    "# Visualize the parameter tuning results\n",
    "plot_parameter_performance(\n",
    "    min_samples_splits_to_test, \n",
    "    [gini_accuracies, entropy_accuracies], \n",
    "    \"Min Samples Split\", \n",
    "    criteria=['gini', 'entropy']\n",
    ")\n",
    "\n",
    "# Find best parameters\n",
    "max_gini_idx = gini_accuracies.index(max(gini_accuracies))\n",
    "max_entropy_idx = entropy_accuracies.index(max(entropy_accuracies))\n",
    "best_gini = min_samples_splits_to_test[max_gini_idx]\n",
    "best_entropy = min_samples_splits_to_test[max_entropy_idx]\n",
    "\n",
    "print(\"\\n--- Best parameters found ---\")\n",
    "print(f\"Best min_samples_split for gini: {best_gini} (accuracy: {max(gini_accuracies):.4f})\")\n",
    "print(f\"Best min_samples_split for entropy: {best_entropy} (accuracy: {max(entropy_accuracies):.4f})\")\n",
    "\n",
    "# Final run with best parameters\n",
    "print(\"\\n--- Final run with best parameters ---\")\n",
    "if max(gini_accuracies) >= max(entropy_accuracies):\n",
    "    best_criterion = 'gini'\n",
    "    best_min_samples = best_gini\n",
    "else:\n",
    "    best_criterion = 'entropy'\n",
    "    best_min_samples = best_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cad60fb",
   "metadata": {},
   "source": [
    "## Parameter Tuning for Knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98494c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_knn(metric='accuracy'):\n",
    "    hist_tests, hist_matrices = test_knn(get_pca_cols(train_df_hist), train_df_hist['label'], get_pca_cols(test_df_hist), test_df_hist['label'])\n",
    "    lbp_tests, lbp_matrices = test_knn(get_pca_cols(train_df_lbp), train_df_lbp['label'], get_pca_cols(test_df_lbp), test_df_lbp['label'])\n",
    "    hog_tests, hog_matrices = test_knn(get_pca_cols(train_df_hog), train_df_hog['label'], get_pca_cols(test_df_hog), test_df_hog['label'])\n",
    "\n",
    "    def find_best(tests: pd.DataFrame, matrices: np.ndarray, title: str, n=3):\n",
    "        top = tests.sort_values(metric, ascending=False).head(n)\n",
    "        display(Markdown(f\"## {title}\"))\n",
    "        display(top)\n",
    "\n",
    "        class_names = ['SPOILED', 'HALF', 'FRESH']\n",
    "        for index, row in top.iterrows():\n",
    "            matrix = matrices[index]\n",
    "\n",
    "            # Visualize confusion matrix.\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.imshow(matrix, interpolation='nearest', cmap=plt.cm.Purples)\n",
    "            plt.suptitle(f\"Knn Confusion Matrix ({title})\")\n",
    "            plt.title(f\"n_neighbors={row['n_neighbors']}  weight={row['weight']}  p={row['p']}\", fontsize='medium')\n",
    "            plt.colorbar()\n",
    "            tick_marks = np.arange(len(class_names))\n",
    "            plt.xticks(tick_marks, class_names, rotation=45)\n",
    "            plt.yticks(tick_marks, class_names)\n",
    "\n",
    "            thresh = matrix.max() / 2\n",
    "            for i in range(matrix.shape[0]):\n",
    "                for j in range(matrix.shape[1]):\n",
    "                    plt.text(j, i, format(matrix[i, j], 'd'),\n",
    "                            horizontalalignment=\"center\",\n",
    "                            color=\"white\" if matrix[i, j] > thresh else \"black\")\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.ylabel('True label')\n",
    "            plt.xlabel('Predicted label')\n",
    "            plt.show()\n",
    "    \n",
    "    find_best(hist_tests, hist_matrices, \"Color Histograms\", 3)\n",
    "    find_best(lbp_tests, lbp_matrices, \"Local Binary Patterns\", 3)\n",
    "    find_best(hog_tests, hog_matrices, \"Histograms of Oriented Gradients\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b4aa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_knn(metric='accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
